{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"General Flow based data-collector and time-centric data-processor. Faxe's inner core is based on a dataflow computing engine and it's components also called nodes can be freely combined into an acyclic graph. Unlike other flowbased frameworks (node_red, ...) in Faxe computing graphs are built with a DSL called dfs . Rest Api FAXE can be managed via its rest api . General Data in faxe In faxe we deal with data_points and data_batches . These data-items are emitted by nodes Every data_point consists of a ts field, fields and tags . The value of the ts field is always: unix-timestamp in millisecond precision without a timezone . fields and tags are essentially key-value maps . Valid data-types for field and tag values are: string, integer, float, key-value map (also deeply nested) and lists . The only valid data-type for field and tag keys is string . A data_batch consists of a list of data_points ordered by timestamp. Most faxe nodes can deal with both points and batches. Value referencing As field and tag values can be deeply nested maps and lists, it is possible to use a JSON-path like syntax to declare and reference these values: Valid examples: averages axis.z.cur value.sub[2].data averages.emitted[5]","title":"General"},{"location":"index.html#general","text":"Flow based data-collector and time-centric data-processor. Faxe's inner core is based on a dataflow computing engine and it's components also called nodes can be freely combined into an acyclic graph. Unlike other flowbased frameworks (node_red, ...) in Faxe computing graphs are built with a DSL called dfs .","title":"General"},{"location":"index.html#rest-api","text":"FAXE can be managed via its rest api .","title":"Rest Api"},{"location":"index.html#general_1","text":"","title":"General"},{"location":"index.html#data-in-faxe","text":"In faxe we deal with data_points and data_batches . These data-items are emitted by nodes Every data_point consists of a ts field, fields and tags . The value of the ts field is always: unix-timestamp in millisecond precision without a timezone . fields and tags are essentially key-value maps . Valid data-types for field and tag values are: string, integer, float, key-value map (also deeply nested) and lists . The only valid data-type for field and tag keys is string . A data_batch consists of a list of data_points ordered by timestamp. Most faxe nodes can deal with both points and batches.","title":"Data in faxe"},{"location":"index.html#value-referencing","text":"As field and tag values can be deeply nested maps and lists, it is possible to use a JSON-path like syntax to declare and reference these values: Valid examples: averages axis.z.cur value.sub[2].data averages.emitted[5]","title":"Value referencing"},{"location":"configuration.html","text":"Configuration TBD FAXE supports a sysctl like configuration syntax. Here are the simple rules of the syntax: Everything you need to know about a single setting is on one line Lines are structured Key = Value Any line starting with # is a comment, and will be ignored. [ {faxe, [ {dfs, [ {script_path, \"/home/ubuntu/faxe/dfs/\" } ]}, {http_api_port, 8081 }, {python, [ {version, \"3\" }, %% python version to use / string ! {script_path, \"/home/ubuntu/faxe/python/\" } ]}, {esq_base_dir, << \"/tmp/\" >> }, {metrics, [ %% {msg_queue_len_high_watermark, 15} %% install metrics handlers {handler, [ {mqtt, [ {host, << \"10.14.204.3\" >> }, {port, 1883 } ] }]} ]}, {conn_status, [ %% install conn_status handlers {handler, [ {mqtt, [ {host, << \"10.14.204.3\" >> }, {port, 2883 }, {user, << \"admin\" >> }, {pass, << \"admin\" >> } ] }]} ]}, {debug_trace, [ {handler, [ {mqtt, [ {host, << \"10.14.204.3\" >> }, {port, 2883 }, %{user, <<\"admin\">>}, %{pass, <<\"admin\">>}, {base_topic, << \"ttgw/sys/faxe/\" >> } ]} ]} ]}, %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %% default configs {s7pool_initial_size, 2 }, {s7pool_max_size, 16 }, {email, [ {from_address, << \"noreply@tgw-group.com\" >> }, {smtp_relay, << \"smtp.tgw.local\" >> }, {smtp_user, undefined}, {smtp_pass, undefined}, {template, << \"/home/ubuntu/faxe/templates/email_template.html\" >> } ]}, {mqtt, [ {host, << \"10.14.204.3\" >> }, {port, 1883 } ]}, {amqp, [ {host, << \"10.14.204.3\" >> }, {port, 5672 } ]}, {rabbitmq, [ {root_exchange, << \"x_lm_fanout\" >> } ]}, {crate, [ {host, << \"10.14.204.8\" >> }, {port, 5433 }, {user, << \"crate\" >> }, {database, << \"doc\" >> } ]}, {crate_http, [ {host, << \"10.14.204.8\" >> }, {port, 4201 } ]} %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ]}, {vmstats, [ {sink, faxe_vmstats}, {interval, 3000 }, {sched_time, false} ] }, {lager, [ {colored, true}, {error_logger_redirect, true}, {handlers, [ {lager_console_backend, [{level, warning}, {formatter, lager_default_formatter}, {formatter_config, [time, color, \" [\" ,severity, \"] \" , {flow, [ \"(\" , flow, \") \" ], [ \"\" ]}, {comp, [ \"(\" , comp, \") \" ], [ \"\" ]}, message, \" \\e [0m \\r\\n \" ] % clear color and newline } ]} , { lager_flowlog_backend, [ {level, notice}, {host, << \"http://10.14.204.8\" >> }, {port, 4201 }, {fields, [ << \"ts\" >> , << \"severity\" >> , << \"flow\" >> , << \"comp\" >> , << \"message\" >> , << \"meta\" >> ] }, {storage_backend, crate_log_writer} ] } , {lager_file_backend, [{file, \"log/error.log\" }, {level, error}]} %% , %% {lager_file_backend, [{file, \"error.log\"}, {level, error}]}, %% {lager_file_backend, [{file, \"console.log\"}, {level, info}]} ]} ]}, {kernel, [ {shell_history, enabled}, {shell_history_path, \".rebar3\" } ] } ].","title":"Configuration"},{"location":"configuration.html#configuration","text":"TBD FAXE supports a sysctl like configuration syntax. Here are the simple rules of the syntax: Everything you need to know about a single setting is on one line Lines are structured Key = Value Any line starting with # is a comment, and will be ignored. [ {faxe, [ {dfs, [ {script_path, \"/home/ubuntu/faxe/dfs/\" } ]}, {http_api_port, 8081 }, {python, [ {version, \"3\" }, %% python version to use / string ! {script_path, \"/home/ubuntu/faxe/python/\" } ]}, {esq_base_dir, << \"/tmp/\" >> }, {metrics, [ %% {msg_queue_len_high_watermark, 15} %% install metrics handlers {handler, [ {mqtt, [ {host, << \"10.14.204.3\" >> }, {port, 1883 } ] }]} ]}, {conn_status, [ %% install conn_status handlers {handler, [ {mqtt, [ {host, << \"10.14.204.3\" >> }, {port, 2883 }, {user, << \"admin\" >> }, {pass, << \"admin\" >> } ] }]} ]}, {debug_trace, [ {handler, [ {mqtt, [ {host, << \"10.14.204.3\" >> }, {port, 2883 }, %{user, <<\"admin\">>}, %{pass, <<\"admin\">>}, {base_topic, << \"ttgw/sys/faxe/\" >> } ]} ]} ]}, %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %% default configs {s7pool_initial_size, 2 }, {s7pool_max_size, 16 }, {email, [ {from_address, << \"noreply@tgw-group.com\" >> }, {smtp_relay, << \"smtp.tgw.local\" >> }, {smtp_user, undefined}, {smtp_pass, undefined}, {template, << \"/home/ubuntu/faxe/templates/email_template.html\" >> } ]}, {mqtt, [ {host, << \"10.14.204.3\" >> }, {port, 1883 } ]}, {amqp, [ {host, << \"10.14.204.3\" >> }, {port, 5672 } ]}, {rabbitmq, [ {root_exchange, << \"x_lm_fanout\" >> } ]}, {crate, [ {host, << \"10.14.204.8\" >> }, {port, 5433 }, {user, << \"crate\" >> }, {database, << \"doc\" >> } ]}, {crate_http, [ {host, << \"10.14.204.8\" >> }, {port, 4201 } ]} %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ]}, {vmstats, [ {sink, faxe_vmstats}, {interval, 3000 }, {sched_time, false} ] }, {lager, [ {colored, true}, {error_logger_redirect, true}, {handlers, [ {lager_console_backend, [{level, warning}, {formatter, lager_default_formatter}, {formatter_config, [time, color, \" [\" ,severity, \"] \" , {flow, [ \"(\" , flow, \") \" ], [ \"\" ]}, {comp, [ \"(\" , comp, \") \" ], [ \"\" ]}, message, \" \\e [0m \\r\\n \" ] % clear color and newline } ]} , { lager_flowlog_backend, [ {level, notice}, {host, << \"http://10.14.204.8\" >> }, {port, 4201 }, {fields, [ << \"ts\" >> , << \"severity\" >> , << \"flow\" >> , << \"comp\" >> , << \"message\" >> , << \"meta\" >> ] }, {storage_backend, crate_log_writer} ] } , {lager_file_backend, [{file, \"log/error.log\" }, {level, error}]} %% , %% {lager_file_backend, [{file, \"error.log\"}, {level, error}]}, %% {lager_file_backend, [{file, \"console.log\"}, {level, info}]} ]} ]}, {kernel, [ {shell_history, enabled}, {shell_history_path, \".rebar3\" } ] } ].","title":"Configuration"},{"location":"metrics.html","text":"Metrics, Connection status, Debug events and Logs For debugging and observability Faxe exposes internal metric as well as connection status events. Furthermore every running flow can emit debugging and log events. All these events can be published to an mqtt broker. Topics and routing keys Topic for the mqtt emitters can be prefixed with the config-value base_topic (see config ). Note: MQTT topics should not start with a / character. type topic base_topic default metrics per node { base_topic }/metrics/{flow_id}/{node_id}/{metric_name} sys/ metrics per flow { base_topic }/metrics/{flow_id} sys/ conn_status { base_topic }/conn_status/{flow_id}/{node_id} sys/ debug per node { base_topic }/debug/{flow_id}/{node_id}/{debug_type} sys/ logs per node { base_topic }/log/{flow_id}/{node_id} sys/ Node Metrics Faxe will collect and periodically emit various metrics to configurable endpoints. Metrics are collected for each individual node and a summery for whole tasks. These are the metrics that will be collected for every node running in a task: metric name description metric fields items_in number of items a node received from other nodes or over the network items_out number of items a node emitted to other nodes or over some network connection processing_errors the number of errors that occurred during processing mem_used memory usage in Kib msg_q_size number of items currently in the node-process' message-queue processing_time time in milliseconds it took the node to process 1 item Nodes that start a network connection have additional metrics (such as the modbus, s7read, mqtt, ... - nodes): metric name description metric fields reading_time the time in milliseconds it took the node to read data from a network port bytes_read the number of bytes read from a network port sending_time the time in milliseconds it took the node to send data to some network endpoint bytes_sent the number of bytes send over the network Examples some metric example datapoints in json-format { \"ts\" : 1592386096330 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"counter\" , \"node_id\" : \"default3\" , \"metric_name\" : \"processing_errors\" , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" , \"counter\" : 0 }} { \"ts\" : 1592386096330 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"histogram\" , \"node_id\" : \"default3\" , \"n\" : 8 , \"min\" : 0.011 , \"metric_name\" : \"processing_time\" , \"mean\" : 0.016625 , \"max\" : 0.028 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} { \"ts\" : 1592386076289 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"gauge\" , \"node_id\" : \"default3\" , \"metric_name\" : \"msg_q_size\" , \"gauge\" : 0 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} { \"ts\" : 1592386076289 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"meter\" , \"one\" : 0.2 , \"node_id\" : \"default3\" , \"metric_name\" : \"items_out\" , \"instant\" : 0.2 , \"five\" : 0.2 , \"fifteen\" : 0.2 , \"count\" : 2 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} Common fields field name meaning data.type the metric type, see table below data.node_id the nodes id data.flow_id id of the flow, the node belongs to Fields by metrics-type metric-type field meaning counter counter total counted number meter instant number of occurrences in the last 5 sec meter one 1 min exponentially weighted moving average meter five 5 min exponentially weighted moving average meter fifteen 15 min exponentially weighted moving average meter count total number gauge gauge point-in-time single value histogram mean mean value histogram min minimum value histogram max maximum value histogram n number of values Flow Metrics For every task there is a summary of the node metrics: { \"ts\" : 1592393302700 , \"id\" : \"00000\" , \"df\" : \"92.002\" , \"data\" :{ \"processing_time\" : 0.129 , \"processing_errors\" : 0 , \"msg_q_size\" : 0 , \"mem_used\" : 53976 , \"items_out\" : 4 , \"items_in\" : 4 , \"flow_id\" : \"e6450a2b-0b71-4d10-8011-67dfac1ce676\" } } Configuration Faxe has 2 different metrics-handlers that can be configured. MQTT and AMQP metrics emitter. See config section for details. Use metrics in tasks Faxe's internal metrics can be used in tasks(flows) with the metrics node. Connection status Faxe tracks the status of every external connection it opens and exposes events. These events can be used in tasks with the conn_status node. They can also be sent to a mqtt and/or amqp broker. Examples connecting to an mqtt-broker on ip 10.14.204.3 and port 2883 ... { \"ts\" : 1592386056299 , \"id\" : \"00000\" , \"df\" : \"92.003\" , \"data\" :{ \"status\" : 2 , \"port\" : 2883 , \"peer\" : \"10.14.204.3\" , \"node_id\" : \"sys\" , \"flow_id\" : \"sys\" , \"connected\" : false , \"conn_type\" : \"mqtt\" }} connected to the mqtt-broker { \"ts\" : 1592386056319 , \"id\" : \"00000\" , \"df\" : \"92.003\" , \"data\" :{ \"status\" : 1 , \"port\" : 1883 , \"peer\" : \"10.14.204.3\" , \"node_id\" : \"sys\" , \"flow_id\" : \"sys\" , \"connected\" : true , \"conn_type\" : \"mqtt\" }} The connection status is represented by the boolean value connected and an enum status . connected status meaning false 0 not connected false 2 connecting true 1 connected Configuration As stated above, FAXE has 2 different conn_status-handlers that can be configured : See config section for details. Debug and Logs For debugging purposes faxe flows can expose events on items going in and out of every node in a flow. Like with metrics and conn_status events, these events can be published to an mqtt/amqp broker. Debug and Log events must be started explicitly and they will be published for a certain configurable amount of time. (This is for debugging purposes). See rest api for how to temporarily activate debugging. See config section for details. Example debug data item: { \"ts\" : 1594627419206 , \"id\" : \"00000\" , \"df\" : \"00.000\" , \"data\" : { \"meta\" : { \"type\" : \"item_in\" , \"port\" : 1 , \"node_id\" : \"win_time6\" , \"flow_id\" : \"trace_test\" }, \"data_item\" : \"{\\\"ts\\\":1594627419205,\\\"id\\\":\\\"00000\\\",\\\"df\\\":\\\"00.000\\\",\\\"data\\\":{\\\"val\\\":4.770044683775623}}\" }} Example log { \"ts\" : 1595317825817 , \"id\" : \"00000\" , \"df\" : \"00.000\" , \"data\" : { \"node_id\" : \"eval26\" , \"meta\" : { \"pid\" : \"<0.1750.0>\" , \"node\" : \"faxe@ubuntu\" , \"module\" : \"df_component\" , \"line\" : 316 , \"function\" : \"handle_info\" , \"application\" : \"faxe\" }, \"message\" : \"'error' in component esp_eval caught when processing item: {1,{data_point,1595317823813,#{<<\\\"esp_avg\\\">> => 6.0685635225505425, <<\\\"factored\\\">> => 3.0342817612752713},#{},<<>>}} -- \\\"\\\\n gen_server:try_dispatch/4 line 637\\\\n df_component:handle_info/2 line 314\\\\n esp_eval:process/3 line 39\\\\n esp_eval:eval/4 line 44\\\\n lists:foldl/3 line 1263\\\\n esp_eval:'-eval/4-fun-0-'/4 line 47\\\\n faxe_lambda:execute/3 line 34\\\\n erlang:'/'(undefined, 2)\\\\nEXIT:badarith\\\"\" , \"level\" : \"error\" , \"flow_id\" : \"script5\" }} A data_point caused an error in an eval node.","title":"Metrics, Connection status, Debug events and Logs"},{"location":"metrics.html#metrics-connection-status-debug-events-and-logs","text":"For debugging and observability Faxe exposes internal metric as well as connection status events. Furthermore every running flow can emit debugging and log events. All these events can be published to an mqtt broker.","title":"Metrics, Connection status, Debug events and Logs"},{"location":"metrics.html#topics-and-routing-keys","text":"Topic for the mqtt emitters can be prefixed with the config-value base_topic (see config ). Note: MQTT topics should not start with a / character. type topic base_topic default metrics per node { base_topic }/metrics/{flow_id}/{node_id}/{metric_name} sys/ metrics per flow { base_topic }/metrics/{flow_id} sys/ conn_status { base_topic }/conn_status/{flow_id}/{node_id} sys/ debug per node { base_topic }/debug/{flow_id}/{node_id}/{debug_type} sys/ logs per node { base_topic }/log/{flow_id}/{node_id} sys/","title":"Topics and routing keys"},{"location":"metrics.html#node-metrics","text":"Faxe will collect and periodically emit various metrics to configurable endpoints. Metrics are collected for each individual node and a summery for whole tasks. These are the metrics that will be collected for every node running in a task: metric name description metric fields items_in number of items a node received from other nodes or over the network items_out number of items a node emitted to other nodes or over some network connection processing_errors the number of errors that occurred during processing mem_used memory usage in Kib msg_q_size number of items currently in the node-process' message-queue processing_time time in milliseconds it took the node to process 1 item Nodes that start a network connection have additional metrics (such as the modbus, s7read, mqtt, ... - nodes): metric name description metric fields reading_time the time in milliseconds it took the node to read data from a network port bytes_read the number of bytes read from a network port sending_time the time in milliseconds it took the node to send data to some network endpoint bytes_sent the number of bytes send over the network","title":"Node Metrics"},{"location":"metrics.html#examples","text":"some metric example datapoints in json-format { \"ts\" : 1592386096330 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"counter\" , \"node_id\" : \"default3\" , \"metric_name\" : \"processing_errors\" , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" , \"counter\" : 0 }} { \"ts\" : 1592386096330 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"histogram\" , \"node_id\" : \"default3\" , \"n\" : 8 , \"min\" : 0.011 , \"metric_name\" : \"processing_time\" , \"mean\" : 0.016625 , \"max\" : 0.028 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} { \"ts\" : 1592386076289 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"gauge\" , \"node_id\" : \"default3\" , \"metric_name\" : \"msg_q_size\" , \"gauge\" : 0 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} { \"ts\" : 1592386076289 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"meter\" , \"one\" : 0.2 , \"node_id\" : \"default3\" , \"metric_name\" : \"items_out\" , \"instant\" : 0.2 , \"five\" : 0.2 , \"fifteen\" : 0.2 , \"count\" : 2 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }}","title":"Examples"},{"location":"metrics.html#common-fields","text":"field name meaning data.type the metric type, see table below data.node_id the nodes id data.flow_id id of the flow, the node belongs to","title":"Common fields"},{"location":"metrics.html#fields-by-metrics-type","text":"metric-type field meaning counter counter total counted number meter instant number of occurrences in the last 5 sec meter one 1 min exponentially weighted moving average meter five 5 min exponentially weighted moving average meter fifteen 15 min exponentially weighted moving average meter count total number gauge gauge point-in-time single value histogram mean mean value histogram min minimum value histogram max maximum value histogram n number of values","title":"Fields by metrics-type"},{"location":"metrics.html#flow-metrics","text":"For every task there is a summary of the node metrics: { \"ts\" : 1592393302700 , \"id\" : \"00000\" , \"df\" : \"92.002\" , \"data\" :{ \"processing_time\" : 0.129 , \"processing_errors\" : 0 , \"msg_q_size\" : 0 , \"mem_used\" : 53976 , \"items_out\" : 4 , \"items_in\" : 4 , \"flow_id\" : \"e6450a2b-0b71-4d10-8011-67dfac1ce676\" } }","title":"Flow Metrics"},{"location":"metrics.html#configuration","text":"Faxe has 2 different metrics-handlers that can be configured. MQTT and AMQP metrics emitter. See config section for details.","title":"Configuration"},{"location":"metrics.html#use-metrics-in-tasks","text":"Faxe's internal metrics can be used in tasks(flows) with the metrics node.","title":"Use metrics in tasks"},{"location":"metrics.html#connection-status","text":"Faxe tracks the status of every external connection it opens and exposes events. These events can be used in tasks with the conn_status node. They can also be sent to a mqtt and/or amqp broker.","title":"Connection status"},{"location":"metrics.html#examples_1","text":"connecting to an mqtt-broker on ip 10.14.204.3 and port 2883 ... { \"ts\" : 1592386056299 , \"id\" : \"00000\" , \"df\" : \"92.003\" , \"data\" :{ \"status\" : 2 , \"port\" : 2883 , \"peer\" : \"10.14.204.3\" , \"node_id\" : \"sys\" , \"flow_id\" : \"sys\" , \"connected\" : false , \"conn_type\" : \"mqtt\" }} connected to the mqtt-broker { \"ts\" : 1592386056319 , \"id\" : \"00000\" , \"df\" : \"92.003\" , \"data\" :{ \"status\" : 1 , \"port\" : 1883 , \"peer\" : \"10.14.204.3\" , \"node_id\" : \"sys\" , \"flow_id\" : \"sys\" , \"connected\" : true , \"conn_type\" : \"mqtt\" }} The connection status is represented by the boolean value connected and an enum status . connected status meaning false 0 not connected false 2 connecting true 1 connected","title":"Examples"},{"location":"metrics.html#configuration_1","text":"As stated above, FAXE has 2 different conn_status-handlers that can be configured : See config section for details.","title":"Configuration"},{"location":"metrics.html#debug-and-logs","text":"For debugging purposes faxe flows can expose events on items going in and out of every node in a flow. Like with metrics and conn_status events, these events can be published to an mqtt/amqp broker. Debug and Log events must be started explicitly and they will be published for a certain configurable amount of time. (This is for debugging purposes). See rest api for how to temporarily activate debugging. See config section for details. Example debug data item: { \"ts\" : 1594627419206 , \"id\" : \"00000\" , \"df\" : \"00.000\" , \"data\" : { \"meta\" : { \"type\" : \"item_in\" , \"port\" : 1 , \"node_id\" : \"win_time6\" , \"flow_id\" : \"trace_test\" }, \"data_item\" : \"{\\\"ts\\\":1594627419205,\\\"id\\\":\\\"00000\\\",\\\"df\\\":\\\"00.000\\\",\\\"data\\\":{\\\"val\\\":4.770044683775623}}\" }} Example log { \"ts\" : 1595317825817 , \"id\" : \"00000\" , \"df\" : \"00.000\" , \"data\" : { \"node_id\" : \"eval26\" , \"meta\" : { \"pid\" : \"<0.1750.0>\" , \"node\" : \"faxe@ubuntu\" , \"module\" : \"df_component\" , \"line\" : 316 , \"function\" : \"handle_info\" , \"application\" : \"faxe\" }, \"message\" : \"'error' in component esp_eval caught when processing item: {1,{data_point,1595317823813,#{<<\\\"esp_avg\\\">> => 6.0685635225505425, <<\\\"factored\\\">> => 3.0342817612752713},#{},<<>>}} -- \\\"\\\\n gen_server:try_dispatch/4 line 637\\\\n df_component:handle_info/2 line 314\\\\n esp_eval:process/3 line 39\\\\n esp_eval:eval/4 line 44\\\\n lists:foldl/3 line 1263\\\\n esp_eval:'-eval/4-fun-0-'/4 line 47\\\\n faxe_lambda:execute/3 line 34\\\\n erlang:'/'(undefined, 2)\\\\nEXIT:badarith\\\"\" , \"level\" : \"error\" , \"flow_id\" : \"script5\" }} A data_point caused an error in an eval node.","title":"Debug and Logs"},{"location":"os_tuning.html","text":"Tune OS Limits Number of file desciptors https://www.rabbitmq.com/install-debian.html#kernel-resource-limits https://github.com/basho/basho_docs/blob/master/content/riak/kv/2.2.3/using/performance/open-files-limit.md#debian--ubuntu https://docs.riak.com/riak/kv/2.2.3/using/performance/open-files-limit/#debian-ubuntu For current session $ ulimit -n 100000 Permanent settings Linux: In /etc/security/limit.conf set these two lines: * soft nofile 65536 * hard nofile 100000 Docker https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file erlang Q flag is used in vm.args +Q Number check actual number used erlang:system_info(port_limit).","title":"Os tuning"},{"location":"os_tuning.html#tune-os-limits","text":"","title":"Tune OS Limits"},{"location":"os_tuning.html#number-of-file-desciptors","text":"https://www.rabbitmq.com/install-debian.html#kernel-resource-limits https://github.com/basho/basho_docs/blob/master/content/riak/kv/2.2.3/using/performance/open-files-limit.md#debian--ubuntu https://docs.riak.com/riak/kv/2.2.3/using/performance/open-files-limit/#debian-ubuntu","title":"Number of file desciptors"},{"location":"os_tuning.html#for-current-session","text":"$ ulimit -n 100000","title":"For current session"},{"location":"os_tuning.html#permanent-settings","text":"Linux: In /etc/security/limit.conf set these two lines: * soft nofile 65536 * hard nofile 100000","title":"Permanent settings"},{"location":"os_tuning.html#docker","text":"https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file","title":"Docker"},{"location":"os_tuning.html#erlang","text":"Q flag is used in vm.args +Q Number","title":"erlang"},{"location":"os_tuning.html#check-actual-number-used","text":"erlang:system_info(port_limit).","title":"check actual number used"},{"location":"dfs_examples/consume_data_rewrite_republish.html","text":"Data cleaning and publishing Consume data from an MQTT-Broker and do some cleaning, at the end republish this data. def topic_in = 'ttgw/grip/rovolutionwels/reasoning/schedulers_ol_log' def topic_out = 'ttgw/data/grip/rovolutionwels/reasoning/schedulers_ol_log' def host = '10.14.204.3' | mqtt_subscribe() . host ( host ) . topic ( topic_in ) . dt_field ( 'UTC-Time' ) . dt_format ( 'float_micro' ) %% here is were we clean data | eval( lambda : int ( \"sku_length\" * 1000 ), lambda : int ( \"sku_width\" * 1000 ), lambda : int ( \"source_lc_quantity\" ), lambda : int ( \"pcs_lost\" ) ) %% overwrite the original fields . as ( 'sku_length' , 'sku_width' , 'source_lc_quantity' , 'pcs_lost' ) | delete( 'UTC-Time' ) % publish the resulting message | mqtt_publish() . host ( host ) . qos ( 1 ) . topic ( topic_out ) . retained () Note: If topic_in = topic_out we create a loop, something we do not want normally.","title":"Consume data rewrite republish"},{"location":"dfs_examples/consume_data_rewrite_republish.html#data-cleaning-and-publishing","text":"Consume data from an MQTT-Broker and do some cleaning, at the end republish this data. def topic_in = 'ttgw/grip/rovolutionwels/reasoning/schedulers_ol_log' def topic_out = 'ttgw/data/grip/rovolutionwels/reasoning/schedulers_ol_log' def host = '10.14.204.3' | mqtt_subscribe() . host ( host ) . topic ( topic_in ) . dt_field ( 'UTC-Time' ) . dt_format ( 'float_micro' ) %% here is were we clean data | eval( lambda : int ( \"sku_length\" * 1000 ), lambda : int ( \"sku_width\" * 1000 ), lambda : int ( \"source_lc_quantity\" ), lambda : int ( \"pcs_lost\" ) ) %% overwrite the original fields . as ( 'sku_length' , 'sku_width' , 'source_lc_quantity' , 'pcs_lost' ) | delete( 'UTC-Time' ) % publish the resulting message | mqtt_publish() . host ( host ) . qos ( 1 ) . topic ( topic_out ) . retained () Note: If topic_in = topic_out we create a loop, something we do not want normally.","title":"Data cleaning and publishing"},{"location":"dfs_examples/python_double.html","text":"Custom python node Using a custom python node called double to double values of a data_batch. Here data is produced every 2s, then accumulated to a data_batch of length 6, the result gets then forwarded to our custom python node, which doubles all values of the field val . dfs | value_emitter() . every ( 2s ) . type ( point ) | win_event() . every ( 6 ) @ double () . field ( 'val' ) . as ( 'double_val' ) | debug() python from faxe import Faxe class Double (Faxe): @staticmethod def options (): opts = [ ( b'field' , b'string' ), ( b'as' , b'string' ) ] return opts def init (self, args): self . fieldname = args[ b'field' ] self . asfieldname = args[ b'as' ] def handle_point (self, point_data): self . emit(self . calc(point_data)) def handle_batch (self, batch_data): out_list = list() for point in batch_data: out_list . append(self . calc(point)) self . emit(out_list) def calc (self, point_dict): point_dict[self . asfieldname] = point_dict[self . fieldname] * 2 return point_dict","title":"Python double"},{"location":"dfs_examples/python_double.html#custom-python-node","text":"Using a custom python node called double to double values of a data_batch. Here data is produced every 2s, then accumulated to a data_batch of length 6, the result gets then forwarded to our custom python node, which doubles all values of the field val .","title":"Custom python node"},{"location":"dfs_examples/python_double.html#dfs","text":"| value_emitter() . every ( 2s ) . type ( point ) | win_event() . every ( 6 ) @ double () . field ( 'val' ) . as ( 'double_val' ) | debug()","title":"dfs"},{"location":"dfs_examples/python_double.html#python","text":"from faxe import Faxe class Double (Faxe): @staticmethod def options (): opts = [ ( b'field' , b'string' ), ( b'as' , b'string' ) ] return opts def init (self, args): self . fieldname = args[ b'field' ] self . asfieldname = args[ b'as' ] def handle_point (self, point_data): self . emit(self . calc(point_data)) def handle_batch (self, batch_data): out_list = list() for point in batch_data: out_list . append(self . calc(point)) self . emit(out_list) def calc (self, point_dict): point_dict[self . asfieldname] = point_dict[self . fieldname] * 2 return point_dict","title":"python"},{"location":"dfs_examples/read_modbus_publish_mqtt.html","text":"Modbus to MQTT In this example we write a simple DFS which reads energy data from a modbus-tcp device periodically and publishes to an mqtt broker. def device_id = 255 def modbus_ip = '127.0.0.1' def mqtt_broker = '10.14.204.3' | modbus() . ip ( '127.0.0.1' ) %% not the default port here . port ( 8899 ) . device ( device_id ) %% we read the values every second . every ( 1s ) %% we read 3 values . function ( 'coils' , 'hregs' , 'iregs' ) %% start addresses . from ( 2127 , 3008 , 104 ) %% amount of data for each value . count ( 1 , 2 , 2 ) %% we want these resulting fieldnames . as ( 'ActiveEnergyConsumption' , 'MaximalCurrentValue' , 'BlindEnergyDelivered' ) %% add some default values to each message | default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'my_id_string' , 1 , '01.010' ) %% publish to mqtt broker | mqtt_publish() . host ( mqtt_broker ) . port ( 1883 ) . qos ( 1 ) . topic ( 'ttopic/energy' ) . retained ()","title":"Read modbus publish mqtt"},{"location":"dfs_examples/read_modbus_publish_mqtt.html#modbus-to-mqtt","text":"In this example we write a simple DFS which reads energy data from a modbus-tcp device periodically and publishes to an mqtt broker. def device_id = 255 def modbus_ip = '127.0.0.1' def mqtt_broker = '10.14.204.3' | modbus() . ip ( '127.0.0.1' ) %% not the default port here . port ( 8899 ) . device ( device_id ) %% we read the values every second . every ( 1s ) %% we read 3 values . function ( 'coils' , 'hregs' , 'iregs' ) %% start addresses . from ( 2127 , 3008 , 104 ) %% amount of data for each value . count ( 1 , 2 , 2 ) %% we want these resulting fieldnames . as ( 'ActiveEnergyConsumption' , 'MaximalCurrentValue' , 'BlindEnergyDelivered' ) %% add some default values to each message | default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'my_id_string' , 1 , '01.010' ) %% publish to mqtt broker | mqtt_publish() . host ( mqtt_broker ) . port ( 1883 ) . qos ( 1 ) . topic ( 'ttopic/energy' ) . retained ()","title":"Modbus to MQTT"},{"location":"dfs_script_language/index.html","text":"Introducing the DFS Script Language Faxe uses a Domain Specific Language(DSL) named dfs (Dataflow Scripting Language) to define dataflow tasks involving the extraction, collection, transformation and loading and writing of data and involving, moreover, the tracking of arbitrary changes and the detection of events within data. Dfs is heavily influenced by InfluxData's TICKScript . Dfs is used in .dfs files to define pipelines and graphs for processing data. The Dfs language is designed to chain together the invocation of data processing operations defined in nodes. At the heart of its's engine, faxe will run an acyclic graph of computing nodes (processes) . DFS Definitions Keywords Word Usage true boolean true false boolean false TRUE boolean true FALSE boolean false lambda: used to denote lambda expression def starts a variable declaration Operators Operator Usage + addition operator - substraction operator / division operator * multiplication operator AND and OR or < less than > greater than =< less than or equal <= less than or equal => greater or equal >= greater or equal == equal != Not equal /= Not equal ! Logical Not rem remainder div integer division These operators are mainly used in Lambda expressions. Chaining operators Operator Usage Example | Used to declare a new node instance and chains it to the node above it (if any) |some_node() |debug() || Used to reference a macro script ||some_macro().some_param(3) . Declares a property (or parameter) call, setting or changing an internal param in the node to which it belongs |log() .file('log1.txt') @ Declares a user defined node written in python. Same as |, but for user defined nodes |some_node() ... @mynode() Variables and literals Variables are declared using the keyword def at the start of a declaration. Variables are immutable and cannot be reassigned new values later on in the script, though they can be used in other declarations and can be passed into functions, property calls and text-templates. Variable declarations def string = 'this is a string !' def text = <<< this is a text with some weird chars :// %& >>> def func = lambda : \"value\" / 3 def meas = 4.44 % A lambda expression as literal def func2 = lambda : int ( meas / 13 ) def an_int = 32342 def a_float = 2131.342 % a chain can also be bound to a declaration def in1 = | mqtt_subscribe() . host ( '127.0.0.1' ) . topic ( 'some/topic' ) % it can then be used like so in1 | debug() Datatypes DFS recognizes six basic types, the type of the literal will be interpreted from its declaration. Type name Description Examples string String type. Single quotes are used for string, string can also be multiline 'this_is_a_string' binary Same as 'string', internally faxe does not have a string type, all strings a binaries 'this_is_a_binary' text Text type. Mostly used where strings are used <<< SELECT MEAN(obj['current']) FROM mytable >>> integer Integer type. Arbitrarily big ints are allowed 123456789987654321, 55 float Floating point number. May be arbitrarily big 12.343422023, 5.6 double Same as float 12.343422023, 5.6 duration A duration literal. See section below. 34s, 500ms, 2d lambda A lambda expression. See extra section in this documentation lambda: str_downcase('BIG') Duration literals Duration literals define a span of time. A duration literal is comprised of two parts: an integer and a duration unit. It is essentially an integer terminated by one or a pair of reserved characters, which represent a unit of time. The following table presents the time units used in declaring duration types. Unit Meaning ms millisecond s second m minute h hour d day w week Internally all time and duration related values are converted to milliseconds. Examples def span = 10s def frequency = 10m def short = 50ms | win_time() . period ( 1h ) . every ( 30m ) Text templates {{ variable_name }} Use def this_portion = 'it' def text_template = 'Some string/text where {{this_portion}} will get replaced' In the above example, after compilation of the dfs script the variable text_template will hold the following value: Some string/text where it will get replaced Text templates can be used in variable declarations like in the above example, they can be used in node-parameter and option-parameter calls. When used in template scripts string/text templates can be very powerful. The variable this_portion could be overwritten with a new value for every instantiation of a template script. There is another version of text-templating which uses a value inside the current data_point, that can be used with some nodes in faxe: {{ \"value_name\" }} | email() . body ( <<< No data since {{ \"datetime\" }} on topic 'ttgw/energy' , last value was {{ \"val\" }}. >>> ) Note: We use double quotes to reference a field in the current data_item. Here the values for datetime and val will be taken from the current data_point in the email node. If a field used in a text_template is not present in the current data_point, the string 'undefined' will be used.","title":"Introducing the DFS Script Language"},{"location":"dfs_script_language/index.html#introducing-the-dfs-script-language","text":"Faxe uses a Domain Specific Language(DSL) named dfs (Dataflow Scripting Language) to define dataflow tasks involving the extraction, collection, transformation and loading and writing of data and involving, moreover, the tracking of arbitrary changes and the detection of events within data. Dfs is heavily influenced by InfluxData's TICKScript . Dfs is used in .dfs files to define pipelines and graphs for processing data. The Dfs language is designed to chain together the invocation of data processing operations defined in nodes. At the heart of its's engine, faxe will run an acyclic graph of computing nodes (processes) .","title":"Introducing the DFS Script Language"},{"location":"dfs_script_language/index.html#dfs-definitions","text":"","title":"DFS Definitions"},{"location":"dfs_script_language/index.html#keywords","text":"Word Usage true boolean true false boolean false TRUE boolean true FALSE boolean false lambda: used to denote lambda expression def starts a variable declaration","title":"Keywords"},{"location":"dfs_script_language/index.html#operators","text":"Operator Usage + addition operator - substraction operator / division operator * multiplication operator AND and OR or < less than > greater than =< less than or equal <= less than or equal => greater or equal >= greater or equal == equal != Not equal /= Not equal ! Logical Not rem remainder div integer division These operators are mainly used in Lambda expressions.","title":"Operators"},{"location":"dfs_script_language/index.html#chaining-operators","text":"Operator Usage Example | Used to declare a new node instance and chains it to the node above it (if any) |some_node() |debug() || Used to reference a macro script ||some_macro().some_param(3) . Declares a property (or parameter) call, setting or changing an internal param in the node to which it belongs |log() .file('log1.txt') @ Declares a user defined node written in python. Same as |, but for user defined nodes |some_node() ... @mynode()","title":"Chaining operators"},{"location":"dfs_script_language/index.html#variables-and-literals","text":"Variables are declared using the keyword def at the start of a declaration. Variables are immutable and cannot be reassigned new values later on in the script, though they can be used in other declarations and can be passed into functions, property calls and text-templates.","title":"Variables and literals"},{"location":"dfs_script_language/index.html#variable-declarations","text":"def string = 'this is a string !' def text = <<< this is a text with some weird chars :// %& >>> def func = lambda : \"value\" / 3 def meas = 4.44 % A lambda expression as literal def func2 = lambda : int ( meas / 13 ) def an_int = 32342 def a_float = 2131.342 % a chain can also be bound to a declaration def in1 = | mqtt_subscribe() . host ( '127.0.0.1' ) . topic ( 'some/topic' ) % it can then be used like so in1 | debug()","title":"Variable declarations"},{"location":"dfs_script_language/index.html#datatypes","text":"DFS recognizes six basic types, the type of the literal will be interpreted from its declaration. Type name Description Examples string String type. Single quotes are used for string, string can also be multiline 'this_is_a_string' binary Same as 'string', internally faxe does not have a string type, all strings a binaries 'this_is_a_binary' text Text type. Mostly used where strings are used <<< SELECT MEAN(obj['current']) FROM mytable >>> integer Integer type. Arbitrarily big ints are allowed 123456789987654321, 55 float Floating point number. May be arbitrarily big 12.343422023, 5.6 double Same as float 12.343422023, 5.6 duration A duration literal. See section below. 34s, 500ms, 2d lambda A lambda expression. See extra section in this documentation lambda: str_downcase('BIG')","title":"Datatypes"},{"location":"dfs_script_language/index.html#duration-literals","text":"Duration literals define a span of time. A duration literal is comprised of two parts: an integer and a duration unit. It is essentially an integer terminated by one or a pair of reserved characters, which represent a unit of time. The following table presents the time units used in declaring duration types. Unit Meaning ms millisecond s second m minute h hour d day w week Internally all time and duration related values are converted to milliseconds.","title":"Duration literals"},{"location":"dfs_script_language/index.html#examples","text":"def span = 10s def frequency = 10m def short = 50ms | win_time() . period ( 1h ) . every ( 30m )","title":"Examples"},{"location":"dfs_script_language/index.html#text-templates","text":"{{ variable_name }}","title":"Text templates"},{"location":"dfs_script_language/index.html#use","text":"def this_portion = 'it' def text_template = 'Some string/text where {{this_portion}} will get replaced' In the above example, after compilation of the dfs script the variable text_template will hold the following value: Some string/text where it will get replaced Text templates can be used in variable declarations like in the above example, they can be used in node-parameter and option-parameter calls. When used in template scripts string/text templates can be very powerful. The variable this_portion could be overwritten with a new value for every instantiation of a template script. There is another version of text-templating which uses a value inside the current data_point, that can be used with some nodes in faxe: {{ \"value_name\" }} | email() . body ( <<< No data since {{ \"datetime\" }} on topic 'ttgw/energy' , last value was {{ \"val\" }}. >>> ) Note: We use double quotes to reference a field in the current data_item. Here the values for datetime and val will be taken from the current data_point in the email node. If a field used in a text_template is not present in the current data_point, the string 'undefined' will be used.","title":"Use"},{"location":"dfs_script_language/lambda_expressions.html","text":"Lambda expressions Overview DFS uses lambda expressions to define transformations on data points as well as define Boolean conditions that act as filters. Lambda expressions wrap mathematical operations, Boolean operations, internal function calls or a combination of all three. All lambda expressions in DFS begin with the lambda: keyword. | where( lambda : \"topic\" == 'ttop/grap/prec' ) In the above example \"topic\" is used to access the value of a field called topic from the current data_point and compared against the string 'ttop/grap/prec' . Note here that literal string values are declared using single quotes, while double quotes are used to access the values of tags and fields. ! As field and tag values can be deeply nested maps and lists, it is possible to use a JSON-path like syntax to reference them: Valid examples: \"averages\" \"axis.z.cur\" \"value.sub[2].data\" \"averages.emitted[5]\" Built-in functions Type conversion With a few exceptions every type can be converted to every other type. Bool bool(a_value) -> true|false Integer int(value) -> integer Float float(value) -> float String string(val) -> string Time functions Every data_point in faxe contains a field called ts , which holds a UTC timestamp in milliseconds. Function Description now() -> integer returns an utc timestamp in milliseconds to_iso8601(ts) -> string converts the timestamp to an ISO8601 datetime string to_rfc3339(ts) -> string converts the timestamp to an RFC3339 datetime string millisecond(ts) -> integer milliseconds within the second [0, 999] second(ts) -> integer second within the minute [0, 59] minute(ts) -> integer minute within the hour [0, 59] hour(ts) -> integer hour within the day [0, 23] day(ts) -> integer day within the month [1, 31] day_of_week(ts) -> integer the weekday with week [1, 7] 1 is monday week(ts) -> integer isoweek-number within year [1, 53] month(ts) -> integer month within the year [1, 12] Example: lambda : hour ( \"ts\" ) >= 8 AND hour ( \"ts\" ) < 19 The above expression evaluates to true if the hour of the day for the data point falls between 08:00 and 19:00. Math functions Function Description abs(x) -> number acos(x) -> float acosh(x) -> float asin(x) -> float asinh(x) -> float atan(x) -> float atan2(y, x) -> float atanh(x) -> float ceil(x) -> float cos(x) -> float cosh(x) -> float exp(x) -> float floor(x) -> float fmod(x, y) -> float log(x) -> float log10(x) -> float log2(x) -> float max(x, y) -> number max(list) -> number min(x, y) -> number min(list) -> number pi() -> float gives pi pow(x, y) -> float round(x) -> integer round a number to an integer round_float(x, precision) -> float round a float (x) with the given precision sin(x) -> float sinh(x) -> float sqrt(x) -> float tan(x) -> float tanh(x) -> float String functions String positions start with index 0. Function Description str_at(x, pos) -> string/undefined Returns the grapheme in the position of the given utf8 string. If position is greater than string length, then it returns undefined. Negative offsets count back from the end of the string. str_capitalize(x) -> string Converts the first character in the given string to uppercase and the remaining to lowercase str_contains(x, contents) -> bool Check if string contains any of the given contents str_downcase(x) -> string Convert all characters on the given string to lowercase str_ends_with(x, suffix) -> string Returns true if string ends with suffix, otherwise false. str_ends_with_any(x, suffixes) -> string Returns true if string ends with any of the suffixes given, otherwise false. str_eqi(x,y) -> bool Compares strings case insensitively str_first(x) -> string/undefined Returns the first grapheme from an utf8 string, undefined if the string is empty str_last(x) -> string/undefined Returns the last grapheme from an utf8 string, undefined if the string is empty str_length(x) -> int Returns the number of unicode graphemes in an utf8 string str_lstrip(x) -> string Returns a string where leading Unicode whitespace has been removed str_lstrip(x, char) -> string Returns a string where leading char have been removed str_normalize/2 str_pad_leading/2 str_pad_leading/3 str_pad_trailing/2 str_pad_trailing/3 str_replace(x, patt, repl) -> string Returns a new string based on subject by replacing the parts matching pattern by replacement. str_replace_leading/3 Replaces all leading occurrences of match by replacement of match in string. str_replace_trailing/3 Replaces all trailing occurrences of match by replacement of match in string. str_replace_prefix(x, match, repl) -> string Replaces prefix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just prepended to string. str_replace_suffix(x, match, repl) -> string Replaces suffix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just appended to string. str_reverse(x) -> string Reverses the given string. str_rstrip(x) -> string Returns a string where trailing Unicode whitespace has been removed str_rstrip(x, char) -> string Returns a string where trailing char have been removed str_slice(x, start, len) -> string Returns a substring starting at the offset given by the first, and a length given by the second param, if offset is negative, count back from end of string. str_split/1 str_split/2 str_split/3 str_split_at/2 str_split_by_any/2 str_split_by_any/3 str_split_by_re/2 str_split_by_re/3 str_starts_with(x, pre) -> bool Returns true if string starts with Prefix str_starts_with_any(x, prefixes) -> bool Returns true if string starts with any of the prefixes given, otherwise false. str_strip(x) -> string Returns a string where leading/trailing Unicode whitespace has been removed str_strip(x, char) -> string Returns a string where leading/trailing char have been removed str_upcase(x) -> string Convert all characters on the given string to uppercase Misc Function Description defined(Key) -> bool whether the given Key is defined in the current data-item undefined(Key) -> bool whether the given Key is NOT defined in the current data-item member(Ele, List) -> bool check for list/set membership not_member(Ele, List) -> bool random(N) -> integer generate a random integer between 1 and N random_real(N) -> float generate a random float between 0.0 and 1.0, that gets multiplied by N mem values are set with the mem node ls_mem(Key) -> any get the single value associated with Key from the flow-memory, ls_mem_list(Key) -> any get the list value associated with Key from the flow-memory ls_mem_set() -> any get the set value associated with Key from the flow-memory Conditional functions If Returns the result of its operands depending on the value of the first argument. The second and third arguments must return the same type. Example: | eval( lambda : if ( \"field.val1\" > threshold AND \"field.val1\" != 0 , 'true' , 'false' )) . as ( 'value' ) The value of the field value in the above example will be the string true or false , depending on the condition passed as the first argument. The if function\u2019s return type is the same type as its second and third arguments. if(condition, true expression, false expression)","title":"Lambda expressions"},{"location":"dfs_script_language/lambda_expressions.html#lambda-expressions","text":"","title":"Lambda expressions"},{"location":"dfs_script_language/lambda_expressions.html#overview","text":"DFS uses lambda expressions to define transformations on data points as well as define Boolean conditions that act as filters. Lambda expressions wrap mathematical operations, Boolean operations, internal function calls or a combination of all three. All lambda expressions in DFS begin with the lambda: keyword. | where( lambda : \"topic\" == 'ttop/grap/prec' ) In the above example \"topic\" is used to access the value of a field called topic from the current data_point and compared against the string 'ttop/grap/prec' . Note here that literal string values are declared using single quotes, while double quotes are used to access the values of tags and fields.","title":"Overview"},{"location":"dfs_script_language/lambda_expressions.html#_1","text":"As field and tag values can be deeply nested maps and lists, it is possible to use a JSON-path like syntax to reference them: Valid examples: \"averages\" \"axis.z.cur\" \"value.sub[2].data\" \"averages.emitted[5]\"","title":"!"},{"location":"dfs_script_language/lambda_expressions.html#built-in-functions","text":"","title":"Built-in functions"},{"location":"dfs_script_language/lambda_expressions.html#type-conversion","text":"With a few exceptions every type can be converted to every other type. Bool bool(a_value) -> true|false Integer int(value) -> integer Float float(value) -> float String string(val) -> string","title":"Type conversion"},{"location":"dfs_script_language/lambda_expressions.html#time-functions","text":"Every data_point in faxe contains a field called ts , which holds a UTC timestamp in milliseconds. Function Description now() -> integer returns an utc timestamp in milliseconds to_iso8601(ts) -> string converts the timestamp to an ISO8601 datetime string to_rfc3339(ts) -> string converts the timestamp to an RFC3339 datetime string millisecond(ts) -> integer milliseconds within the second [0, 999] second(ts) -> integer second within the minute [0, 59] minute(ts) -> integer minute within the hour [0, 59] hour(ts) -> integer hour within the day [0, 23] day(ts) -> integer day within the month [1, 31] day_of_week(ts) -> integer the weekday with week [1, 7] 1 is monday week(ts) -> integer isoweek-number within year [1, 53] month(ts) -> integer month within the year [1, 12] Example: lambda : hour ( \"ts\" ) >= 8 AND hour ( \"ts\" ) < 19 The above expression evaluates to true if the hour of the day for the data point falls between 08:00 and 19:00.","title":"Time functions"},{"location":"dfs_script_language/lambda_expressions.html#math-functions","text":"Function Description abs(x) -> number acos(x) -> float acosh(x) -> float asin(x) -> float asinh(x) -> float atan(x) -> float atan2(y, x) -> float atanh(x) -> float ceil(x) -> float cos(x) -> float cosh(x) -> float exp(x) -> float floor(x) -> float fmod(x, y) -> float log(x) -> float log10(x) -> float log2(x) -> float max(x, y) -> number max(list) -> number min(x, y) -> number min(list) -> number pi() -> float gives pi pow(x, y) -> float round(x) -> integer round a number to an integer round_float(x, precision) -> float round a float (x) with the given precision sin(x) -> float sinh(x) -> float sqrt(x) -> float tan(x) -> float tanh(x) -> float","title":"Math functions"},{"location":"dfs_script_language/lambda_expressions.html#string-functions","text":"String positions start with index 0. Function Description str_at(x, pos) -> string/undefined Returns the grapheme in the position of the given utf8 string. If position is greater than string length, then it returns undefined. Negative offsets count back from the end of the string. str_capitalize(x) -> string Converts the first character in the given string to uppercase and the remaining to lowercase str_contains(x, contents) -> bool Check if string contains any of the given contents str_downcase(x) -> string Convert all characters on the given string to lowercase str_ends_with(x, suffix) -> string Returns true if string ends with suffix, otherwise false. str_ends_with_any(x, suffixes) -> string Returns true if string ends with any of the suffixes given, otherwise false. str_eqi(x,y) -> bool Compares strings case insensitively str_first(x) -> string/undefined Returns the first grapheme from an utf8 string, undefined if the string is empty str_last(x) -> string/undefined Returns the last grapheme from an utf8 string, undefined if the string is empty str_length(x) -> int Returns the number of unicode graphemes in an utf8 string str_lstrip(x) -> string Returns a string where leading Unicode whitespace has been removed str_lstrip(x, char) -> string Returns a string where leading char have been removed str_normalize/2 str_pad_leading/2 str_pad_leading/3 str_pad_trailing/2 str_pad_trailing/3 str_replace(x, patt, repl) -> string Returns a new string based on subject by replacing the parts matching pattern by replacement. str_replace_leading/3 Replaces all leading occurrences of match by replacement of match in string. str_replace_trailing/3 Replaces all trailing occurrences of match by replacement of match in string. str_replace_prefix(x, match, repl) -> string Replaces prefix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just prepended to string. str_replace_suffix(x, match, repl) -> string Replaces suffix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just appended to string. str_reverse(x) -> string Reverses the given string. str_rstrip(x) -> string Returns a string where trailing Unicode whitespace has been removed str_rstrip(x, char) -> string Returns a string where trailing char have been removed str_slice(x, start, len) -> string Returns a substring starting at the offset given by the first, and a length given by the second param, if offset is negative, count back from end of string. str_split/1 str_split/2 str_split/3 str_split_at/2 str_split_by_any/2 str_split_by_any/3 str_split_by_re/2 str_split_by_re/3 str_starts_with(x, pre) -> bool Returns true if string starts with Prefix str_starts_with_any(x, prefixes) -> bool Returns true if string starts with any of the prefixes given, otherwise false. str_strip(x) -> string Returns a string where leading/trailing Unicode whitespace has been removed str_strip(x, char) -> string Returns a string where leading/trailing char have been removed str_upcase(x) -> string Convert all characters on the given string to uppercase","title":"String functions"},{"location":"dfs_script_language/lambda_expressions.html#misc","text":"Function Description defined(Key) -> bool whether the given Key is defined in the current data-item undefined(Key) -> bool whether the given Key is NOT defined in the current data-item member(Ele, List) -> bool check for list/set membership not_member(Ele, List) -> bool random(N) -> integer generate a random integer between 1 and N random_real(N) -> float generate a random float between 0.0 and 1.0, that gets multiplied by N mem values are set with the mem node ls_mem(Key) -> any get the single value associated with Key from the flow-memory, ls_mem_list(Key) -> any get the list value associated with Key from the flow-memory ls_mem_set() -> any get the set value associated with Key from the flow-memory","title":"Misc"},{"location":"dfs_script_language/lambda_expressions.html#conditional-functions","text":"If Returns the result of its operands depending on the value of the first argument. The second and third arguments must return the same type. Example: | eval( lambda : if ( \"field.val1\" > threshold AND \"field.val1\" != 0 , 'true' , 'false' )) . as ( 'value' ) The value of the field value in the above example will be the string true or false , depending on the condition passed as the first argument. The if function\u2019s return type is the same type as its second and third arguments. if(condition, true expression, false expression)","title":"Conditional functions"},{"location":"dfs_script_language/macros.html","text":"Macros are a way to reuse often used parts and to build more complex scripts more easy. Every task that is registered with faxe can be used as a macro-script. Note: Macros are implemented on the script-level, so faxe's internal engine does not know anything about macros. %%% MACRO called 'multiply_above_threshold' def threshold = 30 def factor = 2 | where( lambda : \"value\" > threshold ) | eval( lambda : \"value\" * factor ) . as ( 'multiple' ) We use the macro: | value_emitter() . every ( 500ms ) . type ( point ) || multiply_above_threshold () . threshold ( 2.7 ) A macro is referenced with a double pipe symbol || , followed by the name of the task which we want to use as macro. The resulting script will look like this: | value_emitter() . every ( 500ms ) . type ( point ) def threshold = 2.7 def factor = 2 | where( lambda : \"value\" > threshold ) | eval( lambda : \"value\" * factor ) . as ( 'multiple' ) We can override every literal declaration within the macro-script by simply using them as node-parameters. Ie: here the declaration 'threshold' is overriden, we could also override 'factor'. Theoretically any number of macro-references ||dfs_script_name can be used in a single dfs-script. Furthermore every macro used in a script can itself reference any number of macros, as every macro-script is an ordinary dfs-script registered in faxe .","title":"Macros"},{"location":"dfs_script_language/node_connection.html","text":"Nodes are normally connected by their occurence in the dfs script. | node1 () | node2 () Node1 is connected to node2. The above could also be written as: def n1 = | node1 () n1 | node2 () ... which results in the same computing flow. Here we see that we can actively manipulate the connections in a flow by binding a node to a declaration with the def keyword. Whole chains of nodes ie: sub-graph can be bound to a variable. This is called a chain-declaration . With the above example we can connect another node to n1: def n1 = | node1 () n1 | node2 () n1 | node3 () Here both nodes node2 and node3 are connected to node1. Note: Every use of the def keyword interrupts the auto chaining of nodes, ie: | node1 () | node2 () def n3 = | node3 () | node4 () In the above example, node3 and node4 are not connected to node2, as a consequence of using the def keyword. Instead we have 2 chains in this flow: 1. Node1 connected to node2 and 2. node3 connected node4. If we'd like to union these 2 node chains: def in1 = | node1 () | node2 () def in2 = | node3 () | node4 () in1 | union( in2 ) There are several node-types in faxe that deal with more than one input node, for example the combine node. Here the use of chain-declarations is necessary: def s1 = | node1 () | node1_1 def s2 = | node2 () | node2_1 () s1 | combine( s2 )","title":"Node connection"},{"location":"dfs_script_language/node_connection.html#note","text":"Every use of the def keyword interrupts the auto chaining of nodes, ie: | node1 () | node2 () def n3 = | node3 () | node4 () In the above example, node3 and node4 are not connected to node2, as a consequence of using the def keyword. Instead we have 2 chains in this flow: 1. Node1 connected to node2 and 2. node3 connected node4. If we'd like to union these 2 node chains: def in1 = | node1 () | node2 () def in2 = | node3 () | node4 () in1 | union( in2 ) There are several node-types in faxe that deal with more than one input node, for example the combine node. Here the use of chain-declarations is necessary: def s1 = | node1 () | node1_1 def s2 = | node2 () | node2_1 () s1 | combine( s2 )","title":"Note:"},{"location":"nodes/index.html","text":"Faxe nodes Parameters Faxe nodes can have 2 types of parameters : Node parameters provided to the node declaration function % the level parameter is given to the node declaration function | debug( 'notice' ) Option parameters provided to an option call % the level parameter is given as an extra option function | debug() . level ( 'notice' ) Some parameters are required and others are optional. Every parameter with no default value is mandatory ! The following is a list of all possible parameter types faxe supports based on the basic data-types: Name Description Example is_set Special parameter type that evaluates to true if called (even with no value) .use_ssl() number Integer or float value 324 or 4.3424325 integer Integer value float Floating point value double Floating point value string String value .topic('home/alex/garage') binary atom used internally only list any kind of list lambda a lambda expression bool number_list a list of numbers .values(3, 44, 34.5) integer_list a list of integers .ints(2, 3, 4, 5) float_list a list of floats .floats(43.4, 12.2, 545.009832) string_list a list of strings .strings('alex1', 'alex2', 'flo', 'markus') binary_list atom_list internally only lambda_list a list of lambda expressions .functions(lambda: \"val\" * 2, lambda: \"val\" * 3, lambda: \"val\" / 4) What is important to note: If a node requires a _list type for any parameter, we just provide 1 or more of the same data-type separated be commas. For example the eval node requires the lambdas parameter to be of type lambda_list , the following calls would be valid: | eval( lambda : str_concat ( \"strval\" , '_postfix' ) | eval( lambda : str_starts_with ( \"strval\" , 'pre' ), lambda : 3 * ( \"val1\" + \"val2\" )) | eval( lambda : sqrt ( \"base\" ) + const , lambda : if ( hour ( \"ts\" ) > 18 AND day_of_week ( \"ts\" ) < 6 , 'late_for_work' , 'ok' ), lambda : abs ( \"ts\" - \"ts_previous\" ) )","title":"Faxe nodes"},{"location":"nodes/index.html#faxe-nodes","text":"","title":"Faxe nodes"},{"location":"nodes/index.html#parameters","text":"Faxe nodes can have 2 types of parameters : Node parameters provided to the node declaration function % the level parameter is given to the node declaration function | debug( 'notice' ) Option parameters provided to an option call % the level parameter is given as an extra option function | debug() . level ( 'notice' ) Some parameters are required and others are optional. Every parameter with no default value is mandatory ! The following is a list of all possible parameter types faxe supports based on the basic data-types: Name Description Example is_set Special parameter type that evaluates to true if called (even with no value) .use_ssl() number Integer or float value 324 or 4.3424325 integer Integer value float Floating point value double Floating point value string String value .topic('home/alex/garage') binary atom used internally only list any kind of list lambda a lambda expression bool number_list a list of numbers .values(3, 44, 34.5) integer_list a list of integers .ints(2, 3, 4, 5) float_list a list of floats .floats(43.4, 12.2, 545.009832) string_list a list of strings .strings('alex1', 'alex2', 'flo', 'markus') binary_list atom_list internally only lambda_list a list of lambda expressions .functions(lambda: \"val\" * 2, lambda: \"val\" * 3, lambda: \"val\" / 4) What is important to note: If a node requires a _list type for any parameter, we just provide 1 or more of the same data-type separated be commas. For example the eval node requires the lambdas parameter to be of type lambda_list , the following calls would be valid: | eval( lambda : str_concat ( \"strval\" , '_postfix' ) | eval( lambda : str_starts_with ( \"strval\" , 'pre' ), lambda : 3 * ( \"val1\" + \"val2\" )) | eval( lambda : sqrt ( \"base\" ) + const , lambda : if ( hour ( \"ts\" ) > 18 AND day_of_week ( \"ts\" ) < 6 , 'late_for_work' , 'ok' ), lambda : abs ( \"ts\" - \"ts_previous\" ) )","title":"Parameters"},{"location":"nodes/email.html","text":"The email node Send an email to one or more recipients Example | email() . to ( <<< name @ email. com >>> , <<< another @ email. com >>> ) . subject ( 'Alert #ex3 EnergyData' ) . body ( <<< No data since {{ \"datetime\" }} on topic 'home/garage/energy' , last value was {{ \"val\" }}. >>> ) Sends an email with the subject 'Alert #ex3 EnergyData' to 2 recipients. The body will be rendered into an html template (see parameters). Body is a text_template parameter with two template-values: datetime and val , these two fields must be present in the data_point last received in the email node. If a field used in a text_template is not found in the current data_point, the string 'undefined' will be used. Parameters Parameter Description Default to( string_list ) the recipient email addresses subject( string ) body( text_template ) body_field( string ) field_path used to get the body string template ( string ) html email template to use from config file from_address ( string ) from config file smtp_relay( string ) from config file smtp_user ( string ) from config file smtp_pass ( string ) from config file body or body_field must be provided.","title":"Email"},{"location":"nodes/email.html#the-email-node","text":"Send an email to one or more recipients","title":"The email node"},{"location":"nodes/email.html#example","text":"| email() . to ( <<< name @ email. com >>> , <<< another @ email. com >>> ) . subject ( 'Alert #ex3 EnergyData' ) . body ( <<< No data since {{ \"datetime\" }} on topic 'home/garage/energy' , last value was {{ \"val\" }}. >>> ) Sends an email with the subject 'Alert #ex3 EnergyData' to 2 recipients. The body will be rendered into an html template (see parameters). Body is a text_template parameter with two template-values: datetime and val , these two fields must be present in the data_point last received in the email node. If a field used in a text_template is not found in the current data_point, the string 'undefined' will be used.","title":"Example"},{"location":"nodes/email.html#parameters","text":"Parameter Description Default to( string_list ) the recipient email addresses subject( string ) body( text_template ) body_field( string ) field_path used to get the body string template ( string ) html email template to use from config file from_address ( string ) from config file smtp_relay( string ) from config file smtp_user ( string ) from config file smtp_pass ( string ) from config file body or body_field must be provided.","title":"Parameters"},{"location":"nodes/http_post.html","text":"The http_post node Sends incoming data to a specified HTTP endpoint via the POST method as a JSON message. If any errors occur during the request, the node will attempt to retry sending. Example | http_post() . host ( 'remote.com' ) . port ( 8088 ) . path ( '/receive/json' ) Sends all incoming data to http://remote.com:8088/receive/json in JSON format. Parameters Parameter Description Default host( string ) hostname or ip address of endpoint port( integer ) port number tls( is_set ) whether to use tls ie. https false (not set) path( string ) URI path of the http endpoint ''","title":"Http post"},{"location":"nodes/http_post.html#the-http_post-node","text":"Sends incoming data to a specified HTTP endpoint via the POST method as a JSON message. If any errors occur during the request, the node will attempt to retry sending.","title":"The http_post node"},{"location":"nodes/http_post.html#example","text":"| http_post() . host ( 'remote.com' ) . port ( 8088 ) . path ( '/receive/json' ) Sends all incoming data to http://remote.com:8088/receive/json in JSON format.","title":"Example"},{"location":"nodes/http_post.html#parameters","text":"Parameter Description Default host( string ) hostname or ip address of endpoint port( integer ) port number tls( is_set ) whether to use tls ie. https false (not set) path( string ) URI path of the http endpoint ''","title":"Parameters"},{"location":"nodes/mem.html","text":"The mem node Flow wide value storage. mem values are available to any other node (in lambda expressions) within a flow. There are 3 types of memories: 'single' holds a single value 'list' holds a list of values, value order is preserved within the list 'set' holds a list of values, where values have not duplicates The values will be held in an non persistent ets term storage. Example | mem() . type ( 'set' ) . field ( 'topic' ) . key ( 'topics_seen' ) Holds a set of values from the field named topic . The set of values is available in lambda expression (within the same flow) with the key topics_seen . The above set can be used in lambda expressions with the functions: ls_mem , ls_mem_list , ls_mem_set . | where( lambda : member ( \"topic\" , ls_mem_set ( 'topics_seen' )) ) This will filter out all points that have a topic field, which has already be stored in the mem set. Thus the where node will only output points with a unique topic value. For a list of lambda_library functions see lambda_functions . Parameters Parameter Description Default field( string ) field-path key( string ) name of the value storage type( string ) Type of mem storage, one of 'single', 'list' or 'set' 'single'","title":"Mem"},{"location":"nodes/mem.html#the-mem-node","text":"Flow wide value storage. mem values are available to any other node (in lambda expressions) within a flow. There are 3 types of memories: 'single' holds a single value 'list' holds a list of values, value order is preserved within the list 'set' holds a list of values, where values have not duplicates The values will be held in an non persistent ets term storage.","title":"The mem node"},{"location":"nodes/mem.html#example","text":"| mem() . type ( 'set' ) . field ( 'topic' ) . key ( 'topics_seen' ) Holds a set of values from the field named topic . The set of values is available in lambda expression (within the same flow) with the key topics_seen . The above set can be used in lambda expressions with the functions: ls_mem , ls_mem_list , ls_mem_set . | where( lambda : member ( \"topic\" , ls_mem_set ( 'topics_seen' )) ) This will filter out all points that have a topic field, which has already be stored in the mem set. Thus the where node will only output points with a unique topic value. For a list of lambda_library functions see lambda_functions .","title":"Example"},{"location":"nodes/mem.html#parameters","text":"Parameter Description Default field( string ) field-path key( string ) name of the value storage type( string ) Type of mem storage, one of 'single', 'list' or 'set' 'single'","title":"Parameters"},{"location":"nodes/python.html","text":"The python node Rules for python callback classes: Callback class must be in a module with the lowercase name of the class ie: module: \"double\", class: \"Double\" python callback class must be a subclass of the class Faxe from module faxe 'abstract' methods to implement are (note: they are all optional ): options() -> return a list of tuples // static init(self, args ) -> gets the object and a dict with args from options() handle_point(self, point_data) -> point_data is a dict handle_batch(self, batch_data ) -> batch_data is a list of dicts (points) the callbacks need not return anything except for the options method to emit data the method self.emit(data) has to be used, where data is a dict or a list of dicts A custom python node is used with an @ as node sign instead of | in dfs! @ my_custom_python_node () Parameters Parameters can be freely defined by the python callback class via the static options() method (See example blow). Note that parameter definition must be in bytes type. Example Callback The example python callback class below defined 2 Parameters: field must be a string and has no default value (so it must be given) as must be a string and has the default value 'double' from faxe import Faxe class Double (Faxe): @staticmethod def options (): \"\"\" overwrite this method to request options you would like to use return value is a list of tuples: (option_name, option_type, (optional: default type)) a two tuple: (b\"foo\", b\"string\") with no default value is mandatory in the dfs script a three tuple: (b\"foo\", b\"string\", b\"mystring\") may be overwritten in a dfs script :return: list of tuples \"\"\" opts = [ ( b'field' , b'string' ), ( b'as' , b'string' , b'double' ) ] return opts def init (self, args): \"\"\" will be called on startup with args requested with options() :param args: dict \"\"\" self . fieldname = args[ b'field' ] self . asfieldname = args[ b'as' ] print ( \"my args: \" , args) def handle_point (self, point_data): \"\"\" called when a data_point comes in to this node :param point_data: dict \"\"\" self . emit(self . calc(point_data)) def handle_batch (self, batch_data): \"\"\" called when a data_batch comes in :param batch_data: list of dicts \"\"\" out_list = list() for point in batch_data: out_list . append(self . calc(point)) self . emit(out_list) def calc (self, point_dict): point_dict[self . asfieldname] = point_dict[self . fieldname] * 2 return point_dict Use in a dfs script: @ double () . field ( 'val' ) . as ( 'double_val' )","title":"Python"},{"location":"nodes/python.html#the-python-node","text":"Rules for python callback classes: Callback class must be in a module with the lowercase name of the class ie: module: \"double\", class: \"Double\" python callback class must be a subclass of the class Faxe from module faxe 'abstract' methods to implement are (note: they are all optional ): options() -> return a list of tuples // static init(self, args ) -> gets the object and a dict with args from options() handle_point(self, point_data) -> point_data is a dict handle_batch(self, batch_data ) -> batch_data is a list of dicts (points) the callbacks need not return anything except for the options method to emit data the method self.emit(data) has to be used, where data is a dict or a list of dicts A custom python node is used with an @ as node sign instead of | in dfs! @ my_custom_python_node ()","title":"The python node"},{"location":"nodes/python.html#parameters","text":"Parameters can be freely defined by the python callback class via the static options() method (See example blow). Note that parameter definition must be in bytes type.","title":"Parameters"},{"location":"nodes/python.html#example-callback","text":"The example python callback class below defined 2 Parameters: field must be a string and has no default value (so it must be given) as must be a string and has the default value 'double' from faxe import Faxe class Double (Faxe): @staticmethod def options (): \"\"\" overwrite this method to request options you would like to use return value is a list of tuples: (option_name, option_type, (optional: default type)) a two tuple: (b\"foo\", b\"string\") with no default value is mandatory in the dfs script a three tuple: (b\"foo\", b\"string\", b\"mystring\") may be overwritten in a dfs script :return: list of tuples \"\"\" opts = [ ( b'field' , b'string' ), ( b'as' , b'string' , b'double' ) ] return opts def init (self, args): \"\"\" will be called on startup with args requested with options() :param args: dict \"\"\" self . fieldname = args[ b'field' ] self . asfieldname = args[ b'as' ] print ( \"my args: \" , args) def handle_point (self, point_data): \"\"\" called when a data_point comes in to this node :param point_data: dict \"\"\" self . emit(self . calc(point_data)) def handle_batch (self, batch_data): \"\"\" called when a data_batch comes in :param batch_data: list of dicts \"\"\" out_list = list() for point in batch_data: out_list . append(self . calc(point)) self . emit(out_list) def calc (self, point_dict): point_dict[self . asfieldname] = point_dict[self . fieldname] * 2 return point_dict Use in a dfs script: @ double () . field ( 'val' ) . as ( 'double_val' )","title":"Example Callback"},{"location":"nodes/stats.html","text":"The stats node The stats node lets you compute statistical functions on data_points and data_batches. See nodes under statistics for details. Stats nodes produce a new stream, the incoming stream is not outputted. Parameters All statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Stats"},{"location":"nodes/stats.html#the-stats-node","text":"The stats node lets you compute statistical functions on data_points and data_batches. See nodes under statistics for details. Stats nodes produce a new stream, the incoming stream is not outputted.","title":"The stats node"},{"location":"nodes/stats.html#parameters","text":"All statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/data_collection/modbus.html","text":"The modbus node Pull data via modbus tcp, supported read functions are : ['coils', 'hregs', 'iregs', 'inputs', 'memory'] Reading can be done periodically (if every is given) and/or via a trigger. Read multiple values with possibly different functions at once If the align property is set, the nodes's read times will be truncated to the every property (For example, if the node is started at 12:06 and the every property is 5m then the next read will occur at 12:10, then the next at 12:15 and so on, instead of 12:06, 12:11 and so on). Example | modbus() . ip ( '127.0.0.1' ) . device ( 255 ) . every ( 1s ) . function ( 'coils' , 'hregs' , 'iregs' ) . from ( 2127 , 3008 , 104 ) . count ( 1 , 2 , 2 ) . as ( 'Energy.EnergyConsumption' , 'Energy.CurrentValue' , 'Energy.EnergyDelivered' ) . output ( 'int16' , 'float32' , 'float32' ) . signed ( true , true , false ) Parameters Parameter Description Default ip( string ) ip address of modbus device port( integer ) port of modbus device 502 every( duration ) time between reads undefined align( is_set ) align read intervals according to every false (not set) device( integer ) modbus device id (0-255) 255 function( string_list ) list of read functions, one of ['coils', 'hregs', 'iregs', 'inputs', 'memory'] from( integer_list ) list of start values count( integer_list ) list of count values, how much data to read for every function given as( string_list ) output names for the read values output( string_list ) list of output formats one of ['int16', 'int32', 'float32', 'coils', 'ascii', 'binary'] undefined signed( atom_list true/false) list of values indicating if values are signed undefined Note that, if given, all read parameters( function, from, count, as, output, signed ) must have the same length, this means if you have two values you want to read : . function ( 'coils' , 'hregs' ) ` all corresponding read params (if given) must have the same length: . as ( 'val1' , 'val2' ) . output ( int16 , float32 ) . from ( 1 , 2 ) . count ( 2 , 4 ) . signed ( true , true )","title":"Modbus"},{"location":"nodes/data_collection/modbus.html#the-modbus-node","text":"Pull data via modbus tcp, supported read functions are : ['coils', 'hregs', 'iregs', 'inputs', 'memory'] Reading can be done periodically (if every is given) and/or via a trigger. Read multiple values with possibly different functions at once If the align property is set, the nodes's read times will be truncated to the every property (For example, if the node is started at 12:06 and the every property is 5m then the next read will occur at 12:10, then the next at 12:15 and so on, instead of 12:06, 12:11 and so on).","title":"The modbus node"},{"location":"nodes/data_collection/modbus.html#example","text":"| modbus() . ip ( '127.0.0.1' ) . device ( 255 ) . every ( 1s ) . function ( 'coils' , 'hregs' , 'iregs' ) . from ( 2127 , 3008 , 104 ) . count ( 1 , 2 , 2 ) . as ( 'Energy.EnergyConsumption' , 'Energy.CurrentValue' , 'Energy.EnergyDelivered' ) . output ( 'int16' , 'float32' , 'float32' ) . signed ( true , true , false )","title":"Example"},{"location":"nodes/data_collection/modbus.html#parameters","text":"Parameter Description Default ip( string ) ip address of modbus device port( integer ) port of modbus device 502 every( duration ) time between reads undefined align( is_set ) align read intervals according to every false (not set) device( integer ) modbus device id (0-255) 255 function( string_list ) list of read functions, one of ['coils', 'hregs', 'iregs', 'inputs', 'memory'] from( integer_list ) list of start values count( integer_list ) list of count values, how much data to read for every function given as( string_list ) output names for the read values output( string_list ) list of output formats one of ['int16', 'int32', 'float32', 'coils', 'ascii', 'binary'] undefined signed( atom_list true/false) list of values indicating if values are signed undefined Note that, if given, all read parameters( function, from, count, as, output, signed ) must have the same length, this means if you have two values you want to read : . function ( 'coils' , 'hregs' ) ` all corresponding read params (if given) must have the same length: . as ( 'val1' , 'val2' ) . output ( int16 , float32 ) . from ( 1 , 2 ) . count ( 2 , 4 ) . signed ( true , true )","title":"Parameters"},{"location":"nodes/data_collection/s7poll.html","text":"The s7poll node deprecated: use the s7read node With the s7read node more data can be read in one go. Periodically pull data from a siemens s7 plc via the snap7 library using the iso on tcp protocol . Data addressing can be done in a Step7 schema or with a sligthly different schema used in node-red (although the step7 variant is preferred). See table below for more information. Note: max 19 values can be read with one s7poll node at the moment. Example | s7poll() . ip ( 10.10.204.15 ) . port ( 102 ) . rack ( 0 ) . slot ( 2 ) . every ( 3s ) . vars ( 'DB1140.DBX4.0' , 'DB1140.DBX4.1' , 'DB1140.DBX4.4' , 'DB1140.DBX4.5' ) . as ( 'data.tbo[1].ix_OcM1' , 'data.tbo[1].ix_OcM2' , 'data.tbo[1].ix_Lift_PosTop' , 'data.tbo[1].ix_Lift_PosBo' ) Read 4 values (BOOL in this case) from a plc every 3 seconds and name them with a deep json path. Parameters Parameter Description Default ip( string ) ip address of plc port( integer ) port of modbus device 102 every( duration ) time between reads 1s align( is_set ) align read intervals according to every false (not set) slot( integer ) plc slot number 0 rack( integer ) plc rack number 0 vars( string_list ) list of s7 addresses ie: 'DB3.DBX2.5' (see table below) as( string_list ) output names for the read values diff( is_set ) when given, only output values different to previous values false (not set) Note that params vars and as must have the same length. Data addressing Note: Step7 style preferred and should be used ! Address Step7 equivalent JS Data type Description DB5,X0.1 DB5.DBX0.1 Boolean Bit 1 of byte 0 of DB 5 DB23,B1 or DB23,BYTE1 DB23.DBB1 Number Byte 1 (0-255) of DB 23 DB100,C2 or DB100,CHAR2 DB100.DBB2 String Byte 2 of DB 100 as a Char DB42,I3 or DB42,INT3 DB42.DBW3 Number Signed 16-bit number at byte 3 of DB 42 DB57,WORD4 DB57.DBW4 Number Unsigned 16-bit number at byte 4 of DB 57 DB13,DI5 or DB13,DINT5 DB13.DBD5 Number Signed 32-bit number at byte 5 of DB 13 DB19,DW6 or DB19,DWORD6 DB19.DBD6 Number Unsigned 32-bit number at byte 6 of DB 19 DB21,DR7 or DB21,REAL7 DB19.DBD6 Number Floating point 32-bit number at byte 7 of DB 21 DB2,S7.10* - String String of length 10 starting at byte 7 of DB 2 I1.0 or E1.0 I1.0 or E1.0 Boolean Bit 0 of byte 1 of input area Q2.1 or A2.1 Q2.1 or A2.1 Boolean Bit 1 of byte 2 of output area M3.2 QM3.2 Boolean Bit 2 of byte 3 of memory area IB4 or EB4 IB4 or EB4 Number Byte 4 (0 -255) of input area QB5 or AB5 QB5 or AB5 Number Byte 5 (0 -255) of output area MB6 MB6 Number Byte 6 (0 -255) of memory area IC7 or EC7 IB7 or EB7 String Byte 7 of input area as a Char QC8 or AC8 QB8 or AB8 String Byte 8 of output area as a Char MC9 MB9 String Byte 9 of memory area as a Char II10 or EI10 IW10 or EW10 Number Signed 16-bit number at byte 10 of input area QI12 or AI12 QW12 or AW12 Number Signed 16-bit number at byte 12 of output area MI14 MW14 Number Signed 16-bit number at byte 14 of memory area IW16 or EW16 IW16 or EW16 Number Unsigned 16-bit number at byte 16 of input area QW18 or AW18 QW18 or AW18 Number Unsigned 16-bit number at byte 18 of output area MW20 MW20 Number Unsigned 16-bit number at byte 20 of memory area IDI22 or EDI22 ID22 or ED22 Number Signed 32-bit number at byte 22 of input area QDI24 or ADI24 QD24 or AD24 Number Signed 32-bit number at byte 24 of output area MDI26 MD26 Number Signed 32-bit number at byte 26 of memory area ID28 or ED28 ID28 or ED28 Number Unsigned 32-bit number at byte 28 of input area QD30 or AD30 QD30 or AD30 Number Unsigned 32-bit number at byte 30 of output area MD32 MD32 Number Unsigned 32-bit number at byte 32 of memory area IR34 or ER34 IR34 or ER34 Number Floating point 32-bit number at byte 34 of input area QR36 or AR36 QR36 or AR36 Number Floating point 32-bit number at byte 36 of output area MR38 MR38 Number Floating point 32-bit number at byte 38 of memory area","title":"S7poll"},{"location":"nodes/data_collection/s7poll.html#the-s7poll-node","text":"deprecated: use the s7read node With the s7read node more data can be read in one go. Periodically pull data from a siemens s7 plc via the snap7 library using the iso on tcp protocol . Data addressing can be done in a Step7 schema or with a sligthly different schema used in node-red (although the step7 variant is preferred). See table below for more information. Note: max 19 values can be read with one s7poll node at the moment.","title":"The s7poll node"},{"location":"nodes/data_collection/s7poll.html#example","text":"| s7poll() . ip ( 10.10.204.15 ) . port ( 102 ) . rack ( 0 ) . slot ( 2 ) . every ( 3s ) . vars ( 'DB1140.DBX4.0' , 'DB1140.DBX4.1' , 'DB1140.DBX4.4' , 'DB1140.DBX4.5' ) . as ( 'data.tbo[1].ix_OcM1' , 'data.tbo[1].ix_OcM2' , 'data.tbo[1].ix_Lift_PosTop' , 'data.tbo[1].ix_Lift_PosBo' ) Read 4 values (BOOL in this case) from a plc every 3 seconds and name them with a deep json path.","title":"Example"},{"location":"nodes/data_collection/s7poll.html#parameters","text":"Parameter Description Default ip( string ) ip address of plc port( integer ) port of modbus device 102 every( duration ) time between reads 1s align( is_set ) align read intervals according to every false (not set) slot( integer ) plc slot number 0 rack( integer ) plc rack number 0 vars( string_list ) list of s7 addresses ie: 'DB3.DBX2.5' (see table below) as( string_list ) output names for the read values diff( is_set ) when given, only output values different to previous values false (not set) Note that params vars and as must have the same length.","title":"Parameters"},{"location":"nodes/data_collection/s7poll.html#data-addressing","text":"Note: Step7 style preferred and should be used ! Address Step7 equivalent JS Data type Description DB5,X0.1 DB5.DBX0.1 Boolean Bit 1 of byte 0 of DB 5 DB23,B1 or DB23,BYTE1 DB23.DBB1 Number Byte 1 (0-255) of DB 23 DB100,C2 or DB100,CHAR2 DB100.DBB2 String Byte 2 of DB 100 as a Char DB42,I3 or DB42,INT3 DB42.DBW3 Number Signed 16-bit number at byte 3 of DB 42 DB57,WORD4 DB57.DBW4 Number Unsigned 16-bit number at byte 4 of DB 57 DB13,DI5 or DB13,DINT5 DB13.DBD5 Number Signed 32-bit number at byte 5 of DB 13 DB19,DW6 or DB19,DWORD6 DB19.DBD6 Number Unsigned 32-bit number at byte 6 of DB 19 DB21,DR7 or DB21,REAL7 DB19.DBD6 Number Floating point 32-bit number at byte 7 of DB 21 DB2,S7.10* - String String of length 10 starting at byte 7 of DB 2 I1.0 or E1.0 I1.0 or E1.0 Boolean Bit 0 of byte 1 of input area Q2.1 or A2.1 Q2.1 or A2.1 Boolean Bit 1 of byte 2 of output area M3.2 QM3.2 Boolean Bit 2 of byte 3 of memory area IB4 or EB4 IB4 or EB4 Number Byte 4 (0 -255) of input area QB5 or AB5 QB5 or AB5 Number Byte 5 (0 -255) of output area MB6 MB6 Number Byte 6 (0 -255) of memory area IC7 or EC7 IB7 or EB7 String Byte 7 of input area as a Char QC8 or AC8 QB8 or AB8 String Byte 8 of output area as a Char MC9 MB9 String Byte 9 of memory area as a Char II10 or EI10 IW10 or EW10 Number Signed 16-bit number at byte 10 of input area QI12 or AI12 QW12 or AW12 Number Signed 16-bit number at byte 12 of output area MI14 MW14 Number Signed 16-bit number at byte 14 of memory area IW16 or EW16 IW16 or EW16 Number Unsigned 16-bit number at byte 16 of input area QW18 or AW18 QW18 or AW18 Number Unsigned 16-bit number at byte 18 of output area MW20 MW20 Number Unsigned 16-bit number at byte 20 of memory area IDI22 or EDI22 ID22 or ED22 Number Signed 32-bit number at byte 22 of input area QDI24 or ADI24 QD24 or AD24 Number Signed 32-bit number at byte 24 of output area MDI26 MD26 Number Signed 32-bit number at byte 26 of memory area ID28 or ED28 ID28 or ED28 Number Unsigned 32-bit number at byte 28 of input area QD30 or AD30 QD30 or AD30 Number Unsigned 32-bit number at byte 30 of output area MD32 MD32 Number Unsigned 32-bit number at byte 32 of memory area IR34 or ER34 IR34 or ER34 Number Floating point 32-bit number at byte 34 of input area QR36 or AR36 QR36 or AR36 Number Floating point 32-bit number at byte 36 of output area MR38 MR38 Number Floating point 32-bit number at byte 38 of memory area","title":"Data addressing"},{"location":"nodes/data_collection/s7read.html","text":"The s7read node Read data from a siemens s7 plc via the snap7 library using the iso on tcp protocol . Reading can be done periodically and/or triggered via incoming data-items. If the every parameter is not given, reading will be done only with trigger data-items. Data addressing can be done in a Step7 schema or with a sligthly different schema used in node-red (although the step7 variant is preferred). See table below for more information about addressing. The node will optimize reading by treating contiguous values as one reading var. Thus more data can be read in one go. Connections to a PLC are handled by a connection pool, which can grow and shrink as needed. The number of Min and Max connection count can be set in faxe configuration . Defaults to 2 and 16. Strings A String is defined as a sequence of contiguous char (byte) addresses. For strings faxe uses a special syntax not found in step7 addressing: DB5.DBS7.4 would read 4 bytes starting at byte 7 of DB 5 and output a string value. Note : Characters with an ascii value below 32 will be stripped from the char-sequence. Also for strings the above mentioned read-optimization will not be used. Note: data transfer size is limited to 128 bytes. Examples | s7read () . rack ( 0 ) . slot ( 2 ) . every ( 300ms ) . vars ( 'DB1140.DBX4.0' , 'DB1140.DBX4.1' , 'DB1140.DBX4.4' , 'DB1140.DBX4.5' ) . as ( 'data.tbo[1].ix_OcM1' , 'data.tbo[1].ix_OcM2' , 'data.tbo[1].ix_Lift_PosTop' , 'data.tbo[1].ix_Lift_PosBo' ) Read 4 values (BOOL in this case) from a plc every 3 seconds and name them with a deep json path. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . vars ( 'X4.0' , 'X4.1' , 'X4.4' , 'X4.5' ) . as ( 'ix_OcM1' , 'ix_OcM2' , 'ix_Lift_PosTop' , 'ix_Lift_PosBo' ) Use of as_prefix and vars_prefix, also the s7read node will not read the vars periodically (no every parameter), but only when a trigger in form of any data-item comes in. | s7read () . rack ( 0 ) . slot ( 2 ) . every ( 300ms ) . vars ( 'DB12004.DBS36.30' ) . as ( 'data.LcBc' ) Read a sequence of 30 bytes as a string. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . byte_offset ( 4 ) . vars ( 'X0.0' , 'X0.1' , 'X0.4' , 'X0.5' ) . as ( 'ix_OcM1' , 'ix_OcM2' , 'ix_Lift_PosTop' , 'ix_Lift_PosBo' ) In the last example byte_offset of 4 is used, so effectively the following addresses will be used: X4.0, X4.1, X4.4, X4.5 Parameters Parameter Description Default ip( string ) ip address of plc port( integer ) port of modbus device 102 every( duration ) time between reads undefined align( is_set ) align read intervals according to every false (not set) slot( integer ) plc slot number 0 rack( integer ) plc rack number 0 vars( string_list ) list of s7 addresses ie: 'DB3.DBX2.5' (see table below) as( string_list ) output names for the read values vars_prefix( string ) vars will be prefixed with this value undefined as_prefix( string ) as values will be prefixed with this value undefined byte_offset( integer ) offset for addressing, every address in vars gets this offset added 0 diff( is_set ) when given, only output values different to previous values false (not set) Note that params vars and as must have the same length. Data addressing Note: Step7 style preferred and should be used ! Address Step7 equivalent JS Data type Description DB5,X0.1 DB5.DBX0.1 Boolean Bit 1 of byte 0 of DB 5 DB23,B1 or DB23,BYTE1 DB23.DBB1 Number Byte 1 (0-255) of DB 23 DB100,C2 or DB100,CHAR2 DB100.DBB2 String Byte 2 of DB 100 as a Char DB42,I3 or DB42,INT3 DB42.DBW3 Number Signed 16-bit number at byte 3 of DB 42 DB57,WORD4 DB57.DBW4 Number Unsigned 16-bit number at byte 4 of DB 57 DB13,DI5 or DB13,DINT5 DB13.DBD5 Number Signed 32-bit number at byte 5 of DB 13 DB19,DW6 or DB19,DWORD6 DB19.DBD6 Number Unsigned 32-bit number at byte 6 of DB 19 DB21,DR7 or DB21,REAL7 DB19.DBD6 Number Floating point 32-bit number at byte 7 of DB 21 DB2,S7.10* DB2.DBS7.10 (faxe only) String String of length 10 starting at byte 7 of DB 2 I1.0 or E1.0 I1.0 or E1.0 Boolean Bit 0 of byte 1 of input area Q2.1 or A2.1 Q2.1 or A2.1 Boolean Bit 1 of byte 2 of output area M3.2 QM3.2 Boolean Bit 2 of byte 3 of memory area IB4 or EB4 IB4 or EB4 Number Byte 4 (0 -255) of input area QB5 or AB5 QB5 or AB5 Number Byte 5 (0 -255) of output area MB6 MB6 Number Byte 6 (0 -255) of memory area IC7 or EC7 IB7 or EB7 String Byte 7 of input area as a Char QC8 or AC8 QB8 or AB8 String Byte 8 of output area as a Char MC9 MB9 String Byte 9 of memory area as a Char II10 or EI10 IW10 or EW10 Number Signed 16-bit number at byte 10 of input area QI12 or AI12 QW12 or AW12 Number Signed 16-bit number at byte 12 of output area MI14 MW14 Number Signed 16-bit number at byte 14 of memory area IW16 or EW16 IW16 or EW16 Number Unsigned 16-bit number at byte 16 of input area QW18 or AW18 QW18 or AW18 Number Unsigned 16-bit number at byte 18 of output area MW20 MW20 Number Unsigned 16-bit number at byte 20 of memory area IDI22 or EDI22 ID22 or ED22 Number Signed 32-bit number at byte 22 of input area QDI24 or ADI24 QD24 or AD24 Number Signed 32-bit number at byte 24 of output area MDI26 MD26 Number Signed 32-bit number at byte 26 of memory area ID28 or ED28 ID28 or ED28 Number Unsigned 32-bit number at byte 28 of input area QD30 or AD30 QD30 or AD30 Number Unsigned 32-bit number at byte 30 of output area MD32 MD32 Number Unsigned 32-bit number at byte 32 of memory area IR34 or ER34 IR34 or ER34 Number Floating point 32-bit number at byte 34 of input area QR36 or AR36 QR36 or AR36 Number Floating point 32-bit number at byte 36 of output area MR38 MR38 Number Floating point 32-bit number at byte 38 of memory area","title":"S7read"},{"location":"nodes/data_collection/s7read.html#the-s7read-node","text":"Read data from a siemens s7 plc via the snap7 library using the iso on tcp protocol . Reading can be done periodically and/or triggered via incoming data-items. If the every parameter is not given, reading will be done only with trigger data-items. Data addressing can be done in a Step7 schema or with a sligthly different schema used in node-red (although the step7 variant is preferred). See table below for more information about addressing. The node will optimize reading by treating contiguous values as one reading var. Thus more data can be read in one go. Connections to a PLC are handled by a connection pool, which can grow and shrink as needed. The number of Min and Max connection count can be set in faxe configuration . Defaults to 2 and 16.","title":"The s7read node"},{"location":"nodes/data_collection/s7read.html#strings","text":"A String is defined as a sequence of contiguous char (byte) addresses. For strings faxe uses a special syntax not found in step7 addressing: DB5.DBS7.4 would read 4 bytes starting at byte 7 of DB 5 and output a string value. Note : Characters with an ascii value below 32 will be stripped from the char-sequence. Also for strings the above mentioned read-optimization will not be used. Note: data transfer size is limited to 128 bytes.","title":"Strings"},{"location":"nodes/data_collection/s7read.html#examples","text":"| s7read () . rack ( 0 ) . slot ( 2 ) . every ( 300ms ) . vars ( 'DB1140.DBX4.0' , 'DB1140.DBX4.1' , 'DB1140.DBX4.4' , 'DB1140.DBX4.5' ) . as ( 'data.tbo[1].ix_OcM1' , 'data.tbo[1].ix_OcM2' , 'data.tbo[1].ix_Lift_PosTop' , 'data.tbo[1].ix_Lift_PosBo' ) Read 4 values (BOOL in this case) from a plc every 3 seconds and name them with a deep json path. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . vars ( 'X4.0' , 'X4.1' , 'X4.4' , 'X4.5' ) . as ( 'ix_OcM1' , 'ix_OcM2' , 'ix_Lift_PosTop' , 'ix_Lift_PosBo' ) Use of as_prefix and vars_prefix, also the s7read node will not read the vars periodically (no every parameter), but only when a trigger in form of any data-item comes in. | s7read () . rack ( 0 ) . slot ( 2 ) . every ( 300ms ) . vars ( 'DB12004.DBS36.30' ) . as ( 'data.LcBc' ) Read a sequence of 30 bytes as a string. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . byte_offset ( 4 ) . vars ( 'X0.0' , 'X0.1' , 'X0.4' , 'X0.5' ) . as ( 'ix_OcM1' , 'ix_OcM2' , 'ix_Lift_PosTop' , 'ix_Lift_PosBo' ) In the last example byte_offset of 4 is used, so effectively the following addresses will be used: X4.0, X4.1, X4.4, X4.5","title":"Examples"},{"location":"nodes/data_collection/s7read.html#parameters","text":"Parameter Description Default ip( string ) ip address of plc port( integer ) port of modbus device 102 every( duration ) time between reads undefined align( is_set ) align read intervals according to every false (not set) slot( integer ) plc slot number 0 rack( integer ) plc rack number 0 vars( string_list ) list of s7 addresses ie: 'DB3.DBX2.5' (see table below) as( string_list ) output names for the read values vars_prefix( string ) vars will be prefixed with this value undefined as_prefix( string ) as values will be prefixed with this value undefined byte_offset( integer ) offset for addressing, every address in vars gets this offset added 0 diff( is_set ) when given, only output values different to previous values false (not set) Note that params vars and as must have the same length.","title":"Parameters"},{"location":"nodes/data_collection/s7read.html#data-addressing","text":"Note: Step7 style preferred and should be used ! Address Step7 equivalent JS Data type Description DB5,X0.1 DB5.DBX0.1 Boolean Bit 1 of byte 0 of DB 5 DB23,B1 or DB23,BYTE1 DB23.DBB1 Number Byte 1 (0-255) of DB 23 DB100,C2 or DB100,CHAR2 DB100.DBB2 String Byte 2 of DB 100 as a Char DB42,I3 or DB42,INT3 DB42.DBW3 Number Signed 16-bit number at byte 3 of DB 42 DB57,WORD4 DB57.DBW4 Number Unsigned 16-bit number at byte 4 of DB 57 DB13,DI5 or DB13,DINT5 DB13.DBD5 Number Signed 32-bit number at byte 5 of DB 13 DB19,DW6 or DB19,DWORD6 DB19.DBD6 Number Unsigned 32-bit number at byte 6 of DB 19 DB21,DR7 or DB21,REAL7 DB19.DBD6 Number Floating point 32-bit number at byte 7 of DB 21 DB2,S7.10* DB2.DBS7.10 (faxe only) String String of length 10 starting at byte 7 of DB 2 I1.0 or E1.0 I1.0 or E1.0 Boolean Bit 0 of byte 1 of input area Q2.1 or A2.1 Q2.1 or A2.1 Boolean Bit 1 of byte 2 of output area M3.2 QM3.2 Boolean Bit 2 of byte 3 of memory area IB4 or EB4 IB4 or EB4 Number Byte 4 (0 -255) of input area QB5 or AB5 QB5 or AB5 Number Byte 5 (0 -255) of output area MB6 MB6 Number Byte 6 (0 -255) of memory area IC7 or EC7 IB7 or EB7 String Byte 7 of input area as a Char QC8 or AC8 QB8 or AB8 String Byte 8 of output area as a Char MC9 MB9 String Byte 9 of memory area as a Char II10 or EI10 IW10 or EW10 Number Signed 16-bit number at byte 10 of input area QI12 or AI12 QW12 or AW12 Number Signed 16-bit number at byte 12 of output area MI14 MW14 Number Signed 16-bit number at byte 14 of memory area IW16 or EW16 IW16 or EW16 Number Unsigned 16-bit number at byte 16 of input area QW18 or AW18 QW18 or AW18 Number Unsigned 16-bit number at byte 18 of output area MW20 MW20 Number Unsigned 16-bit number at byte 20 of memory area IDI22 or EDI22 ID22 or ED22 Number Signed 32-bit number at byte 22 of input area QDI24 or ADI24 QD24 or AD24 Number Signed 32-bit number at byte 24 of output area MDI26 MD26 Number Signed 32-bit number at byte 26 of memory area ID28 or ED28 ID28 or ED28 Number Unsigned 32-bit number at byte 28 of input area QD30 or AD30 QD30 or AD30 Number Unsigned 32-bit number at byte 30 of output area MD32 MD32 Number Unsigned 32-bit number at byte 32 of memory area IR34 or ER34 IR34 or ER34 Number Floating point 32-bit number at byte 34 of input area QR36 or AR36 QR36 or AR36 Number Floating point 32-bit number at byte 36 of output area MR38 MR38 Number Floating point 32-bit number at byte 38 of memory area","title":"Data addressing"},{"location":"nodes/data_collection/tcp_receive.html","text":"The tcp_receive node This node connects to a tcp endpoint and awaits data in a special format, which is defined by the parser parameter. The parser will then try to convert the data to faxe's internal format and emit the result. At the moment tcp messages must start with a 2 byte header denoting the length of the following data. `Length_Header:16/integer, Data:{Length_Header}/binary` If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The tcp listener is protected against flooding with the {active, once} inet option. Example def parser = 'parser_robot_plc_v1' | tcp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . parser ( parser ) . as ( 'data' ) Parameters Parameter Description Default ip( string ) ip or hostname for the tcp peer port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set) Available Parsers Parser name Description parser_robot_plc_v1 parses the special robotplc binary data format parser_conv_tracking_v1 parser for the conveyor tracking ascii-protocol parser_wms_rmst_v1 parser_lrep_v1 parser for lrep ascii-protocol","title":"Tcp receive"},{"location":"nodes/data_collection/tcp_receive.html#the-tcp_receive-node","text":"This node connects to a tcp endpoint and awaits data in a special format, which is defined by the parser parameter. The parser will then try to convert the data to faxe's internal format and emit the result. At the moment tcp messages must start with a 2 byte header denoting the length of the following data. `Length_Header:16/integer, Data:{Length_Header}/binary` If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The tcp listener is protected against flooding with the {active, once} inet option.","title":"The tcp_receive node"},{"location":"nodes/data_collection/tcp_receive.html#example","text":"def parser = 'parser_robot_plc_v1' | tcp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . parser ( parser ) . as ( 'data' )","title":"Example"},{"location":"nodes/data_collection/tcp_receive.html#parameters","text":"Parameter Description Default ip( string ) ip or hostname for the tcp peer port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set)","title":"Parameters"},{"location":"nodes/data_collection/tcp_receive.html#available-parsers","text":"Parser name Description parser_robot_plc_v1 parses the special robotplc binary data format parser_conv_tracking_v1 parser for the conveyor tracking ascii-protocol parser_wms_rmst_v1 parser_lrep_v1 parser for lrep ascii-protocol","title":"Available Parsers"},{"location":"nodes/data_collection/tcp_receive_line.html","text":"The tcp_receive_line node This node connects to a tcp endpoint and awaits data in a line separated special format, which is defined by the parser parameter. The parser will then try to convert the data to faxe's internal format and emit the result. At the moment the line separator is fixed to \\n . If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The tcp listener is protected against flooding with the {active, once} inet option. Example def parser = 'parser_conv_tracking_v1' | tcp_recv_line () . ip ( '212.14.149.3' ) . port ( 2004 ) . parser ( parser ) . as ( 'data' ) Parameters Parameter Description Default ip( string ) ip or hostname for the tcp peer port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set) min_length( integer) lines shorter than min_length bytes will be ignored 61 Available Parsers Parser name Description parser_robot_plc_v1 parses the special robotplc binary data format parser_conv_tracking_v1 parser for the conveyor tracking ascii-protocol parser_wms_rmst_v1 parser_lrep_v1 parser for lrep ascii-protocol","title":"Tcp receive line"},{"location":"nodes/data_collection/tcp_receive_line.html#the-tcp_receive_line-node","text":"This node connects to a tcp endpoint and awaits data in a line separated special format, which is defined by the parser parameter. The parser will then try to convert the data to faxe's internal format and emit the result. At the moment the line separator is fixed to \\n . If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The tcp listener is protected against flooding with the {active, once} inet option.","title":"The tcp_receive_line node"},{"location":"nodes/data_collection/tcp_receive_line.html#example","text":"def parser = 'parser_conv_tracking_v1' | tcp_recv_line () . ip ( '212.14.149.3' ) . port ( 2004 ) . parser ( parser ) . as ( 'data' )","title":"Example"},{"location":"nodes/data_collection/tcp_receive_line.html#parameters","text":"Parameter Description Default ip( string ) ip or hostname for the tcp peer port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set) min_length( integer) lines shorter than min_length bytes will be ignored 61","title":"Parameters"},{"location":"nodes/data_collection/tcp_receive_line.html#available-parsers","text":"Parser name Description parser_robot_plc_v1 parses the special robotplc binary data format parser_conv_tracking_v1 parser for the conveyor tracking ascii-protocol parser_wms_rmst_v1 parser_lrep_v1 parser for lrep ascii-protocol","title":"Available Parsers"},{"location":"nodes/data_collection/udp_receive.html","text":"The udp_receive node This node listens on an udp socket and awaits data in a special format, which is defined by the parser parameter, the parser will then try to convert the data to faxe format and emit the result. If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The udp listener is protected against flooding with the {active, once} inet option. Example def parser = 'parser_robot_plc_v1' | udp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . parser ( parser ) . as ( 'data' ) Parameters Parameter Description Default port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set) Available Parsers Parser name Description parser_robot_plc_v1 parses the special robotplc binary data format parser_conv_tracking_v1 parser for the conveyor tracking ascii-protocol","title":"Udp receive"},{"location":"nodes/data_collection/udp_receive.html#the-udp_receive-node","text":"This node listens on an udp socket and awaits data in a special format, which is defined by the parser parameter, the parser will then try to convert the data to faxe format and emit the result. If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The udp listener is protected against flooding with the {active, once} inet option.","title":"The udp_receive node"},{"location":"nodes/data_collection/udp_receive.html#example","text":"def parser = 'parser_robot_plc_v1' | udp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . parser ( parser ) . as ( 'data' )","title":"Example"},{"location":"nodes/data_collection/udp_receive.html#parameters","text":"Parameter Description Default port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set)","title":"Parameters"},{"location":"nodes/data_collection/udp_receive.html#available-parsers","text":"Parser name Description parser_robot_plc_v1 parses the special robotplc binary data format parser_conv_tracking_v1 parser for the conveyor tracking ascii-protocol","title":"Available Parsers"},{"location":"nodes/database/crate_out.html","text":"The crate_out node Write data to CrateDB. Sends data to a CRATE DB HTTP endpoint using Crate's HTTP Api. If any errors occur during the request, the node will attempt to retry sending. Example def db_table = 'grip_log_fulltext3' def db_fields = [ 'id' , 'df' , 'vs' , 'topic' ] def faxe_fields = [ 'id' , 'df' , 'vs' , 'topic' ] | http_post_crate() . table ( db_table ) . db_fields ( db_fields ) . faxe_fields ( faxe_fields ) . remaining_fields_as ( 'data_obj' ) Inserts the faxe-fields id , df , vs , topic into the db-fields with the same names and all remaining fields into the db-field named data_obj (which is of type 'OBJECT') in the table grip_log_fulltext3 . Parameters Parameter Description Default host( string ) hostname or ip address of endpoint from config file port( integer ) port number from config file user( string ) username from config file pass( string ) password from config file tls( is_set ) whether to use tls ie. https false (not set) table( string ) database tablename db_fields( string_list ) db fieldnames (mapping for faxe fieldname to table field names) faxe_fields( string_list ) faxe fieldnames (mapping for faxe fieldname to table field names) remaining_fields_as( string ) if given inserts all fields not in faxe_fields into the given field, which must be of type 'object' undefined","title":"Crate out"},{"location":"nodes/database/crate_out.html#the-crate_out-node","text":"Write data to CrateDB. Sends data to a CRATE DB HTTP endpoint using Crate's HTTP Api. If any errors occur during the request, the node will attempt to retry sending.","title":"The crate_out node"},{"location":"nodes/database/crate_out.html#example","text":"def db_table = 'grip_log_fulltext3' def db_fields = [ 'id' , 'df' , 'vs' , 'topic' ] def faxe_fields = [ 'id' , 'df' , 'vs' , 'topic' ] | http_post_crate() . table ( db_table ) . db_fields ( db_fields ) . faxe_fields ( faxe_fields ) . remaining_fields_as ( 'data_obj' ) Inserts the faxe-fields id , df , vs , topic into the db-fields with the same names and all remaining fields into the db-field named data_obj (which is of type 'OBJECT') in the table grip_log_fulltext3 .","title":"Example"},{"location":"nodes/database/crate_out.html#parameters","text":"Parameter Description Default host( string ) hostname or ip address of endpoint from config file port( integer ) port number from config file user( string ) username from config file pass( string ) password from config file tls( is_set ) whether to use tls ie. https false (not set) table( string ) database tablename db_fields( string_list ) db fieldnames (mapping for faxe fieldname to table field names) faxe_fields( string_list ) faxe fieldnames (mapping for faxe fieldname to table field names) remaining_fields_as( string ) if given inserts all fields not in faxe_fields into the given field, which must be of type 'object' undefined","title":"Parameters"},{"location":"nodes/database/crate_query.html","text":"The crate_query node Query the CRATE database for time series data . This node is experimental . The select statement will be executed periodically according to the every parameter. Each time the database is queried, the timestamps will be set according to period . Example def host = '10.14.204.8' def port = 5433 def query = <<< SELECT avg(data_obj['x']['cur']) AS x_cur, avg(data_obj['y']['cur']) AS y_cur, avg(data_obj['z']['cur']) AS z_cur, avg(data_obj['yaw']['cur']) AS yaw_cur, avg(data_obj['pitch']['cur']) AS pitch_cur FROM robotplc_parted; >>> def s = | crate_query() . query ( query ) . group_by_time ( 3m ) . every ( 15s ) . period ( 30m ) . align () The above example will execute the query every 15 seconds. It get data which is in the timerange now -30 minutes and now . Parameters Parameter Description Default host( string ) CrateDB host from config file port( integer ) CrateDB port from config file user( string ) username from config file pass( string ) password from config file database( string ) Database name from config file query( string text ) 'SELECT-FROM' query clause time_field( string ) name of the timefield to use 'ts' every( duration ) time between query execution 5s period( duration ) time span of data to query 1h align( is_set ) whether to align period to full every durations false (not set) group_by_time( duration ) group the aggregations into time buckets 2m group_by( string_list ) additional group by [] limit( string ) LIMIT statement '30'","title":"Crate query"},{"location":"nodes/database/crate_query.html#the-crate_query-node","text":"Query the CRATE database for time series data . This node is experimental . The select statement will be executed periodically according to the every parameter. Each time the database is queried, the timestamps will be set according to period .","title":"The crate_query node"},{"location":"nodes/database/crate_query.html#example","text":"def host = '10.14.204.8' def port = 5433 def query = <<< SELECT avg(data_obj['x']['cur']) AS x_cur, avg(data_obj['y']['cur']) AS y_cur, avg(data_obj['z']['cur']) AS z_cur, avg(data_obj['yaw']['cur']) AS yaw_cur, avg(data_obj['pitch']['cur']) AS pitch_cur FROM robotplc_parted; >>> def s = | crate_query() . query ( query ) . group_by_time ( 3m ) . every ( 15s ) . period ( 30m ) . align () The above example will execute the query every 15 seconds. It get data which is in the timerange now -30 minutes and now .","title":"Example"},{"location":"nodes/database/crate_query.html#parameters","text":"Parameter Description Default host( string ) CrateDB host from config file port( integer ) CrateDB port from config file user( string ) username from config file pass( string ) password from config file database( string ) Database name from config file query( string text ) 'SELECT-FROM' query clause time_field( string ) name of the timefield to use 'ts' every( duration ) time between query execution 5s period( duration ) time span of data to query 1h align( is_set ) whether to align period to full every durations false (not set) group_by_time( duration ) group the aggregations into time buckets 2m group_by( string_list ) additional group by [] limit( string ) LIMIT statement '30'","title":"Parameters"},{"location":"nodes/database/http_post_crate.html","text":"The http_post_crate node renamed to crate_out .","title":"Http post crate"},{"location":"nodes/database/http_post_crate.html#the-http_post_crate-node","text":"renamed to crate_out .","title":"The http_post_crate node"},{"location":"nodes/database/influx_out.html","text":"The influx_out node Write data to InfluxDB via it's HTTP API. This node supports InfluxDB up to version 1.8. If any errors occur during the request, the node will attempt to retry sending. Example | influx_out () . host ( '127.0.0.1' ) . port ( 8086 ) . measurement ( 'm1' ) . database ( 'mydb' ) Parameters Parameter Description Default host( string ) hostname or ip address of endpoint from config file port( integer ) port number from config file user( string ) username from config file pass( string ) password from config file tls( is_set ) whether to use tls ie. https false (not set) database( string ) database name measurement( string ) measurement name retpol( string ) retention policy to write to default","title":"Influx out"},{"location":"nodes/database/influx_out.html#the-influx_out-node","text":"Write data to InfluxDB via it's HTTP API. This node supports InfluxDB up to version 1.8. If any errors occur during the request, the node will attempt to retry sending.","title":"The influx_out node"},{"location":"nodes/database/influx_out.html#example","text":"| influx_out () . host ( '127.0.0.1' ) . port ( 8086 ) . measurement ( 'm1' ) . database ( 'mydb' )","title":"Example"},{"location":"nodes/database/influx_out.html#parameters","text":"Parameter Description Default host( string ) hostname or ip address of endpoint from config file port( integer ) port number from config file user( string ) username from config file pass( string ) password from config file tls( is_set ) whether to use tls ie. https false (not set) database( string ) database name measurement( string ) measurement name retpol( string ) retention policy to write to default","title":"Parameters"},{"location":"nodes/database/oracle_query.html","text":"The oracle_query node Used to batch a number of points. As soon as the node has collected size points it will emit them in a data_batch. A timeout can be set, after which all points currently in the buffer will be emitted, regardless of the number of collected points. The timeout is started on the first datapoint coming in to an empty buffer. Example | batch( 12 ) | batch( 5 ) . timeout ( 3s ) The second example will output a batch with 5 points. If the points come in within 3 seconds the node will emit them in a databatch and reset the timeout. If after 3 seconds there are less than 5 points in the buffer, the node will emit them, regardless of the number. Parameters Parameter Description Default [node] size( integer ) Number of points to batch timeout( duration ) Previous values TTL optional","title":"Oracle query"},{"location":"nodes/database/oracle_query.html#the-oracle_query-node","text":"Used to batch a number of points. As soon as the node has collected size points it will emit them in a data_batch. A timeout can be set, after which all points currently in the buffer will be emitted, regardless of the number of collected points. The timeout is started on the first datapoint coming in to an empty buffer.","title":"The oracle_query node"},{"location":"nodes/database/oracle_query.html#example","text":"| batch( 12 ) | batch( 5 ) . timeout ( 3s ) The second example will output a batch with 5 points. If the points come in within 3 seconds the node will emit them in a databatch and reset the timeout. If after 3 seconds there are less than 5 points in the buffer, the node will emit them, regardless of the number.","title":"Example"},{"location":"nodes/database/oracle_query.html#parameters","text":"Parameter Description Default [node] size( integer ) Number of points to batch timeout( duration ) Previous values TTL optional","title":"Parameters"},{"location":"nodes/debug/conn_status.html","text":"The conn_status node Subscribe to internal connection status events of running tasks. For more information on these events make yourself familiar with faxe's metrics . Example %% get connection status events for task \"flow1\" and node \"amqp_publish13\" | conn_status () . flow ( 'flow1' ) . node ( 'amqp_publish13' ) %% get connection status events for every node in task \"flow1\" | conn_status () . flow ( 'flow1' ) Parameters Parameter Description Default flow( string ) Id of task node( string ) Id of node undefined","title":"Conn status"},{"location":"nodes/debug/conn_status.html#the-conn_status-node","text":"Subscribe to internal connection status events of running tasks. For more information on these events make yourself familiar with faxe's metrics .","title":"The conn_status node"},{"location":"nodes/debug/conn_status.html#example","text":"%% get connection status events for task \"flow1\" and node \"amqp_publish13\" | conn_status () . flow ( 'flow1' ) . node ( 'amqp_publish13' ) %% get connection status events for every node in task \"flow1\" | conn_status () . flow ( 'flow1' )","title":"Example"},{"location":"nodes/debug/conn_status.html#parameters","text":"Parameter Description Default flow( string ) Id of task node( string ) Id of node undefined","title":"Parameters"},{"location":"nodes/debug/debug.html","text":"The debug node The debug node logs all incoming data with erlang's lager framework and emits it, without touching it. Where the logs will be written, depends on the lager config. See rest api for how to read the produced logs. Example | debug() | debug( 'error' ) Parameters Parameter Description Default [node] level ( string ) log level (see below) 'notice' The level parameter must have one of the following values: log_level debug info notice warning error critical alert","title":"Debug"},{"location":"nodes/debug/debug.html#the-debug-node","text":"The debug node logs all incoming data with erlang's lager framework and emits it, without touching it. Where the logs will be written, depends on the lager config. See rest api for how to read the produced logs.","title":"The debug node"},{"location":"nodes/debug/debug.html#example","text":"| debug() | debug( 'error' )","title":"Example"},{"location":"nodes/debug/debug.html#parameters","text":"Parameter Description Default [node] level ( string ) log level (see below) 'notice' The level parameter must have one of the following values: log_level debug info notice warning error critical alert","title":"Parameters"},{"location":"nodes/debug/json_emitter.html","text":"The json_emitter node This node is for debugging and testing/mocking purposes. It periodically emits one of the json strings given with the json parameter. Example | json_emitter() . every ( 3s ) . json ( <<< {\"condition\": {\"id\": 0, \"name\": \"OK\"}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} >>>, <<< {\"condition\": {\"id\": 1, \"name\": \"Warning\"}, \"condition_reason\": \"bad succer\", \"predicted_maintenance_time\": 1583246411783, \"vac_on_without_contact\": [0.2, 2.5, 8.01], \"vac_on_with_contact\": [6.001, 4.798, 7.012]} >>>, <<< {\"condition\": {\"id\": 2, \"name\": \"Error\"}, \"condition_reason\": \"something went really wrong!\", \"predicted_maintenance_time\": 1582246411783, \"vac_on_without_contact\": [0.5, 2.5, 0.44], \"vac_on_with_contact\": [2.06, 4.98, 2.901]} >>> ) | debug() Emit one of the three given json strings(selected randomly) every 3 seconds. Parameters Parameter Description Default every( duration ) emit interval 5s jitter( duration ) add time jitter to the values produced 0ms json( string_list ) list of json strings align( is_set ) align the time to the every param false (not set)","title":"Json emitter"},{"location":"nodes/debug/json_emitter.html#the-json_emitter-node","text":"This node is for debugging and testing/mocking purposes. It periodically emits one of the json strings given with the json parameter.","title":"The json_emitter node"},{"location":"nodes/debug/json_emitter.html#example","text":"| json_emitter() . every ( 3s ) . json ( <<< {\"condition\": {\"id\": 0, \"name\": \"OK\"}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} >>>, <<< {\"condition\": {\"id\": 1, \"name\": \"Warning\"}, \"condition_reason\": \"bad succer\", \"predicted_maintenance_time\": 1583246411783, \"vac_on_without_contact\": [0.2, 2.5, 8.01], \"vac_on_with_contact\": [6.001, 4.798, 7.012]} >>>, <<< {\"condition\": {\"id\": 2, \"name\": \"Error\"}, \"condition_reason\": \"something went really wrong!\", \"predicted_maintenance_time\": 1582246411783, \"vac_on_without_contact\": [0.5, 2.5, 0.44], \"vac_on_with_contact\": [2.06, 4.98, 2.901]} >>> ) | debug() Emit one of the three given json strings(selected randomly) every 3 seconds.","title":"Example"},{"location":"nodes/debug/json_emitter.html#parameters","text":"Parameter Description Default every( duration ) emit interval 5s jitter( duration ) add time jitter to the values produced 0ms json( string_list ) list of json strings align( is_set ) align the time to the every param false (not set)","title":"Parameters"},{"location":"nodes/debug/log.html","text":"The log node Log incoming data to a file in json format (line by line) Example | log( 'topics.txt' ) Parameters Parameter Description Default [node] file( string ) valid writeable filepath","title":"Log"},{"location":"nodes/debug/log.html#the-log-node","text":"Log incoming data to a file in json format (line by line)","title":"The log node"},{"location":"nodes/debug/log.html#example","text":"| log( 'topics.txt' )","title":"Example"},{"location":"nodes/debug/log.html#parameters","text":"Parameter Description Default [node] file( string ) valid writeable filepath","title":"Parameters"},{"location":"nodes/debug/metrics.html","text":"The metrics node Subscribe to internal metric events of running tasks. For more information on these events make yourself familiar with faxe's metrics . Note: It is not possible to subscribe to metrics for the task the metrics node belongs to. Example %% get all metrics for task \"flow1\" and node \"amqp_publish13\" | metrics () . flow ( 'flow1' ) . node ( 'amqp_publish13' ) %% get total number of bytes read and written for task \"flow32\" | metrics () . flow ( 'flow32' ) . metrics ( 'bytes_read' , 'bytes_sent' ) Parameters Parameter Description Default flow( string ) Id of task node( string ) Id of node undefined metrics( string_list ) List of metric_names undefined","title":"Metrics"},{"location":"nodes/debug/metrics.html#the-metrics-node","text":"Subscribe to internal metric events of running tasks. For more information on these events make yourself familiar with faxe's metrics . Note: It is not possible to subscribe to metrics for the task the metrics node belongs to.","title":"The metrics node"},{"location":"nodes/debug/metrics.html#example","text":"%% get all metrics for task \"flow1\" and node \"amqp_publish13\" | metrics () . flow ( 'flow1' ) . node ( 'amqp_publish13' ) %% get total number of bytes read and written for task \"flow32\" | metrics () . flow ( 'flow32' ) . metrics ( 'bytes_read' , 'bytes_sent' )","title":"Example"},{"location":"nodes/debug/metrics.html#parameters","text":"Parameter Description Default flow( string ) Id of task node( string ) Id of node undefined metrics( string_list ) List of metric_names undefined","title":"Parameters"},{"location":"nodes/debug/value_emitter.html","text":"The value_emitter node This node is for debugging purposes. It periodically emits random values. Example | value_emitter() . every ( 1s ) . type ( point ) Emit a data_point with a random value in field val every second. Parameters Parameter Description Default every( duration ) emit interval 5s jitter( duration ) add time jitter to the values produced 0ms type( atom ) emit point or batch batch fields( string_list ) what fields to emit ['val'] format( atom ) the format of the fields emitted flat/ejson flat align( is_set ) align the time to the every param false (not set)","title":"Value emitter"},{"location":"nodes/debug/value_emitter.html#the-value_emitter-node","text":"This node is for debugging purposes. It periodically emits random values.","title":"The value_emitter node"},{"location":"nodes/debug/value_emitter.html#example","text":"| value_emitter() . every ( 1s ) . type ( point ) Emit a data_point with a random value in field val every second.","title":"Example"},{"location":"nodes/debug/value_emitter.html#parameters","text":"Parameter Description Default every( duration ) emit interval 5s jitter( duration ) add time jitter to the values produced 0ms type( atom ) emit point or batch batch fields( string_list ) what fields to emit ['val'] format( atom ) the format of the fields emitted flat/ejson flat align( is_set ) align the time to the every param false (not set)","title":"Parameters"},{"location":"nodes/flow/combine.html","text":"The combine node Combine the values from 2 nodes, used to enrich a stream of data with data from another stream, that has lower frequency. Port 1 is the trigger port and its the port where data to be enriched comes into the node. Port 2 is the one where enrichment data come in. Every time a value is received on the trigger port, the node will emit a value, combined with whatever current value on port 2. The node will never emit on port 2 values. No output is given, as long as there has not arrived a value on port 2 to combine with. fields The fields parameter defines the fields to inject into the combination for the stream on port 2. To rename these fields, parameter prefix or aliases can be used. With prefix_delimiter a delimiter can be given, defaults to: '_' merge When merge_field is given, the node will merge the values from the input port 2 with the values from port 1. Objects and lists and lists of objects will be merged. If a path exists in both streams, the value in the first stream is superseded by the value in the second stream (in2). Except for lists, which will be combined. Either fields(optionally with prefix or aliases ) or merge_field must be given. If you want to join 2 or more streams consider using the join node . Examples def in1 = | value_emitter() . every ( 500ms ) . type ( point ) . fields ( 'val' ) def in2 = | value_emitter() . every ( 4s ) . type ( point ) . fields ( 'val2' , 'val3' ) in1 | combine( in2 ) . fields ( 'val2' , 'val3' ) . prefix ( 'comb' ) . prefix_delimiter ( '_' ) In this example values from the stream called in1 will be enriched with values from in2 . Outputfields will be called: val , comb_val2 and comb_val3 . The flow will emit every 500 milliseconds after 4 seconds have past initially. def in1 = | value_emitter() . every ( 500ms ) . type ( point ) . fields ( 'data.val' ) def in2 = | value_emitter() . every ( 4s ) . type ( point ) . fields ( 'data.val2' , 'data.val3' ) in1 | combine( in2 ) . merge_field ( 'data' ) This example will merge data from in2 into in1 , such that the resulting data-point will have the fields: data.val , data.val2 , data.val3 def v1 = | json_emitter() . every ( 1s ) . json ( <<< { \"condition\" : { \"id\" : 0 , \"name\" : \"OK\" , \"sub_cond\" : [{ \"value\" : 33 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 1.2 , 2.5 , 4.33 ]} >>> ) def v2 = | json_emitter() . every ( 5s ) . json ( <<< { \"condition\" : { \"id1\" : 0 , \"name1\" : \"OK\" , \"sub_cond\" : [{ \"number\" : 44 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" : [ 2.2 , 2.5 , 4.33 ], \"vac_on_with_contact\" : [ 5.6 , 45.98 , 7.012 ]} >>> ) v1 | combine( v2 ) . merge_field ( 'data' ) The output from the above example will be: {\"data\": {\"condition\": {\"id1\":0,\"id\":0,\"name1\":\"OK\",\"name\":\"OK\", \"sub_cond\":[{\"number\":44},{\"value\":33}]}, \"condition_reason\":\"\", \"predicted_maintenance_time\":1584246411785, \"vac_on_without_contact\":[1.2,2.2,2.5,2.5,4.33,4.33], \"vac_on_with_contact\":[5.6,45.98,7.012] } Parameters Parameter Description Default [node] ( port ) input node for port 2 merge_field( string ) Base field for the merge operation [] fields( string_list ) List of fields to include [] tags( string_list ) List of tags to include [] aliases( string_list ) List of field aliases to use instead of the original field names [] prefix( string ) Prefix for the injected fields from stream 2 undefined prefix_delimiter( string ) Used to separate prefix and the original field name from stream 2 '_' Either prefix or aliases must be given these are mutually exclusive parameters. If both are given, then prefix will win.","title":"Combine"},{"location":"nodes/flow/combine.html#the-combine-node","text":"Combine the values from 2 nodes, used to enrich a stream of data with data from another stream, that has lower frequency. Port 1 is the trigger port and its the port where data to be enriched comes into the node. Port 2 is the one where enrichment data come in. Every time a value is received on the trigger port, the node will emit a value, combined with whatever current value on port 2. The node will never emit on port 2 values. No output is given, as long as there has not arrived a value on port 2 to combine with.","title":"The combine node"},{"location":"nodes/flow/combine.html#fields","text":"The fields parameter defines the fields to inject into the combination for the stream on port 2. To rename these fields, parameter prefix or aliases can be used. With prefix_delimiter a delimiter can be given, defaults to: '_'","title":"fields"},{"location":"nodes/flow/combine.html#merge","text":"When merge_field is given, the node will merge the values from the input port 2 with the values from port 1. Objects and lists and lists of objects will be merged. If a path exists in both streams, the value in the first stream is superseded by the value in the second stream (in2). Except for lists, which will be combined. Either fields(optionally with prefix or aliases ) or merge_field must be given. If you want to join 2 or more streams consider using the join node .","title":"merge"},{"location":"nodes/flow/combine.html#examples","text":"def in1 = | value_emitter() . every ( 500ms ) . type ( point ) . fields ( 'val' ) def in2 = | value_emitter() . every ( 4s ) . type ( point ) . fields ( 'val2' , 'val3' ) in1 | combine( in2 ) . fields ( 'val2' , 'val3' ) . prefix ( 'comb' ) . prefix_delimiter ( '_' ) In this example values from the stream called in1 will be enriched with values from in2 . Outputfields will be called: val , comb_val2 and comb_val3 . The flow will emit every 500 milliseconds after 4 seconds have past initially. def in1 = | value_emitter() . every ( 500ms ) . type ( point ) . fields ( 'data.val' ) def in2 = | value_emitter() . every ( 4s ) . type ( point ) . fields ( 'data.val2' , 'data.val3' ) in1 | combine( in2 ) . merge_field ( 'data' ) This example will merge data from in2 into in1 , such that the resulting data-point will have the fields: data.val , data.val2 , data.val3 def v1 = | json_emitter() . every ( 1s ) . json ( <<< { \"condition\" : { \"id\" : 0 , \"name\" : \"OK\" , \"sub_cond\" : [{ \"value\" : 33 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 1.2 , 2.5 , 4.33 ]} >>> ) def v2 = | json_emitter() . every ( 5s ) . json ( <<< { \"condition\" : { \"id1\" : 0 , \"name1\" : \"OK\" , \"sub_cond\" : [{ \"number\" : 44 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" : [ 2.2 , 2.5 , 4.33 ], \"vac_on_with_contact\" : [ 5.6 , 45.98 , 7.012 ]} >>> ) v1 | combine( v2 ) . merge_field ( 'data' ) The output from the above example will be: {\"data\": {\"condition\": {\"id1\":0,\"id\":0,\"name1\":\"OK\",\"name\":\"OK\", \"sub_cond\":[{\"number\":44},{\"value\":33}]}, \"condition_reason\":\"\", \"predicted_maintenance_time\":1584246411785, \"vac_on_without_contact\":[1.2,2.2,2.5,2.5,4.33,4.33], \"vac_on_with_contact\":[5.6,45.98,7.012] }","title":"Examples"},{"location":"nodes/flow/combine.html#parameters","text":"Parameter Description Default [node] ( port ) input node for port 2 merge_field( string ) Base field for the merge operation [] fields( string_list ) List of fields to include [] tags( string_list ) List of tags to include [] aliases( string_list ) List of field aliases to use instead of the original field names [] prefix( string ) Prefix for the injected fields from stream 2 undefined prefix_delimiter( string ) Used to separate prefix and the original field name from stream 2 '_' Either prefix or aliases must be given these are mutually exclusive parameters. If both are given, then prefix will win.","title":"Parameters"},{"location":"nodes/flow/join.html","text":"The join node Join data from two or more nodes, given a list of prefixes, for each row. If the merge_field parameter is given, the node will merge the field given from every in-node, instead of joining. When considering the fill option, the following rules apply: 'none' - (default) skip rows where a point is missing, inner join. 'null' - fill missing points with null, full outer join. Any value - fill fields with given value, full outer join. Note, that this node will produce a completely new stream. If you want to enrich a stream of data with a second stream consider using the combine node . Example def v1 = | value_emitter() . every ( 3s ) . type ( point ) . align () def v2 = | value_emitter() . every ( 5s ) . type ( point ) . align () v1 | join( v2 ) . prefix ( 'v1.joined' , 'v2.joined' ) . tolerance ( 3s ) . missing_timeout ( 3s ) . fill ( none ) Joins the fields of v1 and v2 and produces a stream, that has the fields v1.joined.val and v2.joined.val Node Parameters Parameter Description Default [node] nodes( node_list ) list of node (chains) to merge [] Parameters Parameter Description Default prefix( string_list ) list of prefixes (used in join mode) ['', ''] (no prefixes) merge_field( string ) when given, the join node will do a field merge operation undefined missing_timeout( duration ) values that do not arrive within this timeout will be treated as missing 20s tolerance( duration ) db fieldnames (mapping for faxe fieldname to table field names) fill( 'none' 'null' any ) fill missing values / join behaviour 'none' Merge example Let's look at an example where the streams coming out of two nodes are not joined with prefixes, but a merge operation is performed. def v1 = | json_emitter() . every ( 3s ) . json ( <<< { \"condition\" : { \"id\" : 0 , \"name\" : \"OK\" , \"sub_cond\" : [{ \"value\" : 33 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 1.2 , 2.5 , 4.33 ]} >>> ) def v2 = | json_emitter() . every ( 3s ) . json ( <<< { \"condition\" : { \"id1\" : 0 , \"name1\" : \"OK\" , \"sub_cond\" : [{ \"number\" : 44 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 2.2 , 2.5 , 4.33 ], \"vac_on_with_contact\" : [ 5.6 , 45.98 , 7.012 ]} >>> ) v1 | join( v2 ) . merge_field ( 'data' ) . tolerance ( 20ms ) . missing_timeout ( 30ms ) . fill ( null ) | debug() v1 node data-field (in json format for readability): { \"condition\" : { \"id\" : 0 , \"name\" : \"OK\" , \"sub_cond\" : [{ \"value\" : 33 }]}, \"condition_reason\" : \"Reason\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 1.2 , 2.5 , 4.33 ] } v2 node data-field: { \"condition\" : { \"id1\" : 0 , \"name1\" : \"OK\" , \"sub_cond\" : [{ \"number\" : 44 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" : [ 2.2 , 2.5 , 4.33 ], \"vac_on_with_contact\" : [ 5.6 , 45.98 , 7.012 ] } The result data-field after merge (json format here): { \"condition\" : { \"name1\" : \"OK\" , \"name\" : \"OK\" , \"id1\" : 0 , \"id\" : 0 , \"sub_cond\" :[{ \"number\" : 44 }, { \"value\" : 33 }] }, \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" :[ 1.2 , 2.2 , 2.5 , 2.5 , 4.33 , 4.33 ], \"vac_on_with_contact\" :[ 5.6 , 45.98 , 7.012 ], \"condition_reason\" : \"\" } Objects and lists and lists of objects will be merged. If a path exists in several streams, the value in the first stream is superseded by the value in a following stream (\"condition_reason\" and \"predicted_maintenance_time\" in this example). Except for lists, which will be merged (\"vac_on_without_contact\").","title":"Join"},{"location":"nodes/flow/join.html#the-join-node","text":"Join data from two or more nodes, given a list of prefixes, for each row. If the merge_field parameter is given, the node will merge the field given from every in-node, instead of joining. When considering the fill option, the following rules apply: 'none' - (default) skip rows where a point is missing, inner join. 'null' - fill missing points with null, full outer join. Any value - fill fields with given value, full outer join. Note, that this node will produce a completely new stream. If you want to enrich a stream of data with a second stream consider using the combine node .","title":"The join node"},{"location":"nodes/flow/join.html#example","text":"def v1 = | value_emitter() . every ( 3s ) . type ( point ) . align () def v2 = | value_emitter() . every ( 5s ) . type ( point ) . align () v1 | join( v2 ) . prefix ( 'v1.joined' , 'v2.joined' ) . tolerance ( 3s ) . missing_timeout ( 3s ) . fill ( none ) Joins the fields of v1 and v2 and produces a stream, that has the fields v1.joined.val and v2.joined.val","title":"Example"},{"location":"nodes/flow/join.html#node-parameters","text":"Parameter Description Default [node] nodes( node_list ) list of node (chains) to merge []","title":"Node Parameters"},{"location":"nodes/flow/join.html#parameters","text":"Parameter Description Default prefix( string_list ) list of prefixes (used in join mode) ['', ''] (no prefixes) merge_field( string ) when given, the join node will do a field merge operation undefined missing_timeout( duration ) values that do not arrive within this timeout will be treated as missing 20s tolerance( duration ) db fieldnames (mapping for faxe fieldname to table field names) fill( 'none' 'null' any ) fill missing values / join behaviour 'none'","title":"Parameters"},{"location":"nodes/flow/join.html#merge-example","text":"Let's look at an example where the streams coming out of two nodes are not joined with prefixes, but a merge operation is performed. def v1 = | json_emitter() . every ( 3s ) . json ( <<< { \"condition\" : { \"id\" : 0 , \"name\" : \"OK\" , \"sub_cond\" : [{ \"value\" : 33 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 1.2 , 2.5 , 4.33 ]} >>> ) def v2 = | json_emitter() . every ( 3s ) . json ( <<< { \"condition\" : { \"id1\" : 0 , \"name1\" : \"OK\" , \"sub_cond\" : [{ \"number\" : 44 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 2.2 , 2.5 , 4.33 ], \"vac_on_with_contact\" : [ 5.6 , 45.98 , 7.012 ]} >>> ) v1 | join( v2 ) . merge_field ( 'data' ) . tolerance ( 20ms ) . missing_timeout ( 30ms ) . fill ( null ) | debug()","title":"Merge example"},{"location":"nodes/flow/join.html#v1-node-data-field-in-json-format-for-readability","text":"{ \"condition\" : { \"id\" : 0 , \"name\" : \"OK\" , \"sub_cond\" : [{ \"value\" : 33 }]}, \"condition_reason\" : \"Reason\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 1.2 , 2.5 , 4.33 ] }","title":"v1 node data-field (in json format for readability):"},{"location":"nodes/flow/join.html#v2-node-data-field","text":"{ \"condition\" : { \"id1\" : 0 , \"name1\" : \"OK\" , \"sub_cond\" : [{ \"number\" : 44 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" : [ 2.2 , 2.5 , 4.33 ], \"vac_on_with_contact\" : [ 5.6 , 45.98 , 7.012 ] } The result data-field after merge (json format here): { \"condition\" : { \"name1\" : \"OK\" , \"name\" : \"OK\" , \"id1\" : 0 , \"id\" : 0 , \"sub_cond\" :[{ \"number\" : 44 }, { \"value\" : 33 }] }, \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" :[ 1.2 , 2.2 , 2.5 , 2.5 , 4.33 , 4.33 ], \"vac_on_with_contact\" :[ 5.6 , 45.98 , 7.012 ], \"condition_reason\" : \"\" } Objects and lists and lists of objects will be merged. If a path exists in several streams, the value in the first stream is superseded by the value in a following stream (\"condition_reason\" and \"predicted_maintenance_time\" in this example). Except for lists, which will be merged (\"vac_on_without_contact\").","title":"v2 node data-field:"},{"location":"nodes/flow/union.html","text":"The union node Union of multiple streams. The union node takes the union of all of its parents as a simple pass through. Data points received from each parent are passed onto child nodes without modification. Example in1 | union( in2 , in3 ) The union of 3 nodes (chain expressions) def in1 = | mqtt_subscribe() . host ... def in2 = | amqp_consume() . host .... def in3 = ... in1 | union( in2 , in3 ) with chain expressions Parameters Parameter Description Default [node] nodes_in( node_list ) optional","title":"Union"},{"location":"nodes/flow/union.html#the-union-node","text":"Union of multiple streams. The union node takes the union of all of its parents as a simple pass through. Data points received from each parent are passed onto child nodes without modification.","title":"The union node"},{"location":"nodes/flow/union.html#example","text":"in1 | union( in2 , in3 ) The union of 3 nodes (chain expressions) def in1 = | mqtt_subscribe() . host ... def in2 = | amqp_consume() . host .... def in3 = ... in1 | union( in2 , in3 ) with chain expressions","title":"Example"},{"location":"nodes/flow/union.html#parameters","text":"Parameter Description Default [node] nodes_in( node_list ) optional","title":"Parameters"},{"location":"nodes/flowdata/batch.html","text":"The batch node Used to batch a number of points. As soon as the node has collected size number of points it will emit them in a data_batch. A timeout can be set, after which all points currently in the buffer will be emitted, regardless of the number of collected points. The timeout is started on the first datapoint coming in to an empty buffer. Example | batch( 12 ) | batch( 5 ) . timeout ( 3s ) The second example will output a batch with 5 points. If the points come in within 3 seconds the node will emit them in a databatch and reset the timeout. If after 3 seconds there are less than 5 points in the buffer, the node will emit them, regardless of the number. Parameters Parameter Description Default [node] size( integer ) Number of points to batch timeout( duration ) 1h","title":"Batch"},{"location":"nodes/flowdata/batch.html#the-batch-node","text":"Used to batch a number of points. As soon as the node has collected size number of points it will emit them in a data_batch. A timeout can be set, after which all points currently in the buffer will be emitted, regardless of the number of collected points. The timeout is started on the first datapoint coming in to an empty buffer.","title":"The batch node"},{"location":"nodes/flowdata/batch.html#example","text":"| batch( 12 ) | batch( 5 ) . timeout ( 3s ) The second example will output a batch with 5 points. If the points come in within 3 seconds the node will emit them in a databatch and reset the timeout. If after 3 seconds there are less than 5 points in the buffer, the node will emit them, regardless of the number.","title":"Example"},{"location":"nodes/flowdata/batch.html#parameters","text":"Parameter Description Default [node] size( integer ) Number of points to batch timeout( duration ) 1h","title":"Parameters"},{"location":"nodes/flowdata/default.html","text":"The default node Add fields and/or tags to a data_point or batch if they do not already exist. Does not overwrite or update any fields or tags. Note: This nodes checks for existence of fields before writing them. Consider using the set node , if you just want some fields set. It is more performant especially with high frequency data streams. Example | default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'some_id' , 1 , '05.043' ) The above example will set the field id to the value 'some_id' , if a field with the name id does not already exist. Accordingly vs will be set to 1, df will be set to '05.043'. Parameters Parameter Description Default fields( string_list ) list of fieldnames [] field_values( list ) list of values for the given fields (must have the same length as fieldnames) [] tags( string_list ) list of tagnames [] tag_values( list ) list of values for the given tags (must have the same length as tagnames) []","title":"Default"},{"location":"nodes/flowdata/default.html#the-default-node","text":"Add fields and/or tags to a data_point or batch if they do not already exist. Does not overwrite or update any fields or tags. Note: This nodes checks for existence of fields before writing them. Consider using the set node , if you just want some fields set. It is more performant especially with high frequency data streams.","title":"The default node"},{"location":"nodes/flowdata/default.html#example","text":"| default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'some_id' , 1 , '05.043' ) The above example will set the field id to the value 'some_id' , if a field with the name id does not already exist. Accordingly vs will be set to 1, df will be set to '05.043'.","title":"Example"},{"location":"nodes/flowdata/default.html#parameters","text":"Parameter Description Default fields( string_list ) list of fieldnames [] field_values( list ) list of values for the given fields (must have the same length as fieldnames) [] tags( string_list ) list of tagnames [] tag_values( list ) list of values for the given tags (must have the same length as tagnames) []","title":"Parameters"},{"location":"nodes/flowdata/delete.html","text":"The delete node Delete fields and/or tags from a data_point or from all data_points in a data_batch. Example | delete() . fields ( 'temp' , 'data.meta[3]' ) The above example will delete the field named temp and the third array entry of the field data.meta . Parameters Parameter Description Default fields( string_list ) list of fieldnames to delete [] tags( string_list ) list of tagnames to delete []","title":"Delete"},{"location":"nodes/flowdata/delete.html#the-delete-node","text":"Delete fields and/or tags from a data_point or from all data_points in a data_batch.","title":"The delete node"},{"location":"nodes/flowdata/delete.html#example","text":"| delete() . fields ( 'temp' , 'data.meta[3]' ) The above example will delete the field named temp and the third array entry of the field data.meta .","title":"Example"},{"location":"nodes/flowdata/delete.html#parameters","text":"Parameter Description Default fields( string_list ) list of fieldnames to delete [] tags( string_list ) list of tagnames to delete []","title":"Parameters"},{"location":"nodes/flowdata/keep.html","text":"The keep node Keep only those fields and tags specified by the parameters. Example | keep( 'data.topic' , 'data.temperature' ) . as ( 'topic' , 'temperature' ) Parameters Parameter Description Default [node] fields( string_list ) list of fieldnames to keep from the incoming data tags( string_list ) list of tagnames to keep from the incoming data [] as( string_list ) list of new field names for the kept fields, if given, must have the same count of names as fields []","title":"Keep"},{"location":"nodes/flowdata/keep.html#the-keep-node","text":"Keep only those fields and tags specified by the parameters.","title":"The keep node"},{"location":"nodes/flowdata/keep.html#example","text":"| keep( 'data.topic' , 'data.temperature' ) . as ( 'topic' , 'temperature' )","title":"Example"},{"location":"nodes/flowdata/keep.html#parameters","text":"Parameter Description Default [node] fields( string_list ) list of fieldnames to keep from the incoming data tags( string_list ) list of tagnames to keep from the incoming data [] as( string_list ) list of new field names for the kept fields, if given, must have the same count of names as fields []","title":"Parameters"},{"location":"nodes/flowdata/rename.html","text":"The rename node Rename existing fields and/or tags. Example | rename() . fields ( 'topic' , 'temperature' ) . as_fields ( 'cipot' , 'mean_temp' ) Parameters Parameter Description Default fields( string_list ) list of fieldnames to rename [] as_fields( string_list ) list of new fieldnames for renaming [] tags( string_list ) list of tagnames to rename [] as_tags( string_list ) list of new tagnames for renaming []","title":"Rename"},{"location":"nodes/flowdata/rename.html#the-rename-node","text":"Rename existing fields and/or tags.","title":"The rename node"},{"location":"nodes/flowdata/rename.html#example","text":"| rename() . fields ( 'topic' , 'temperature' ) . as_fields ( 'cipot' , 'mean_temp' )","title":"Example"},{"location":"nodes/flowdata/rename.html#parameters","text":"Parameter Description Default fields( string_list ) list of fieldnames to rename [] as_fields( string_list ) list of new fieldnames for renaming [] tags( string_list ) list of tagnames to rename [] as_tags( string_list ) list of new tagnames for renaming []","title":"Parameters"},{"location":"nodes/flowdata/set.html","text":"The set node Set fields and/or tags to a data_point or batch. Overwrites any existing fields or tags. If fields or tags should be written only if they do not already exist, use the default node . Example | set() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'some_id' , 1 , '05.043' ) The above example will set the field id to the value 'some_id'. Accordingly vs will be set to 1, df will be set to '05.043'. Parameters Parameter Description Default fields( string_list ) list of fieldnames [] field_values( list ) list of values for the given fields (must have the same length as fieldnames) [] tags( string_list ) list of tagnames [] tag_values( list ) list of values for the given tags (must have the same length as tagnames) []","title":"Set"},{"location":"nodes/flowdata/set.html#the-set-node","text":"Set fields and/or tags to a data_point or batch. Overwrites any existing fields or tags. If fields or tags should be written only if they do not already exist, use the default node .","title":"The set node"},{"location":"nodes/flowdata/set.html#example","text":"| set() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'some_id' , 1 , '05.043' ) The above example will set the field id to the value 'some_id'. Accordingly vs will be set to 1, df will be set to '05.043'.","title":"Example"},{"location":"nodes/flowdata/set.html#parameters","text":"Parameter Description Default fields( string_list ) list of fieldnames [] field_values( list ) list of values for the given fields (must have the same length as fieldnames) [] tags( string_list ) list of tagnames [] tag_values( list ) list of values for the given tags (must have the same length as tagnames) []","title":"Parameters"},{"location":"nodes/logic/case.html","text":"The case node Evaluates a series of lambda expressions in a top down manner. The node will output / add the corresponding value of the first lambda expression that evaluates as true. If none of the lambda expressions evaluate as true, a default value will be used The case node works in a similar way CASE expressions in SQL work. Example | case( lambda : \"data.condition.name\" == 'OK' , lambda : \"data.condition.name\" == 'Warning' , lambda : \"data.condition.name\" == 'Error' ) . values ( <<<{\"cond\": \"Everything OK!\"}>>>, <<<{\"cond\": \"Oh, oh, a Warning!\"}>>>, <<<{\"cond\": \"Damn, Error!\"}>>> ) . json () . as ( 'data' ) .default( <<< { \"cond\" : \"Nothing matched!!!\" } >>> ) Parameters Parameter Description Default [node] lambdas( lambda_list ) list of lambda expressions values( string_list\\|text_list ) corresponding values json( is_set ) if set, will treat the values and default parameters as json strings false, not set as ( string ) field-path for the output value default( any ) default value to use, if no case clause matches","title":"Case"},{"location":"nodes/logic/case.html#the-case-node","text":"Evaluates a series of lambda expressions in a top down manner. The node will output / add the corresponding value of the first lambda expression that evaluates as true. If none of the lambda expressions evaluate as true, a default value will be used The case node works in a similar way CASE expressions in SQL work.","title":"The case node"},{"location":"nodes/logic/case.html#example","text":"| case( lambda : \"data.condition.name\" == 'OK' , lambda : \"data.condition.name\" == 'Warning' , lambda : \"data.condition.name\" == 'Error' ) . values ( <<<{\"cond\": \"Everything OK!\"}>>>, <<<{\"cond\": \"Oh, oh, a Warning!\"}>>>, <<<{\"cond\": \"Damn, Error!\"}>>> ) . json () . as ( 'data' ) .default( <<< { \"cond\" : \"Nothing matched!!!\" } >>> )","title":"Example"},{"location":"nodes/logic/case.html#parameters","text":"Parameter Description Default [node] lambdas( lambda_list ) list of lambda expressions values( string_list\\|text_list ) corresponding values json( is_set ) if set, will treat the values and default parameters as json strings false, not set as ( string ) field-path for the output value default( any ) default value to use, if no case clause matches","title":"Parameters"},{"location":"nodes/logic/change_detect.html","text":"The change_detect node Emits new point-values only if different from the previous point. Multiple fields can be monitored for change by this node. If no fields are given, the complete data-item is compared to the last one. If a reset_timeout is given, all previous values will be reset when no value is received within this amount of time. So that after the timeout the first data_item will be emitted. For value comparison erlang's strict equals (=:=) is used, so 1.0 is not equal to 1. Example %% detects any changes in data_items | change_detect() %% detect changes in one field, with timeout %% outputs at least the first data_item coming in after a 3 second timeout | change_detect( 'val' ) . reset_timeout ( 3s ) %% detect changes in two fields, with timeout | change_detect( 'data.State.Err' , 'data.State.Msg' ) . reset_timeout ( 60s ) % in-example json notation: % {\"data\": {\"x\": {\"temp\": 32.4564}, \"y\" : {\"temp\" : 31.15155}} } | change_detect( 'data.x.temp' , 'data.y.temp' ) Parameters Parameter Description Default [node] fields( string_list ) List of fields to monitor optional reset_timeout( duration ) Previous values TTL 3h","title":"Change detect"},{"location":"nodes/logic/change_detect.html#the-change_detect-node","text":"Emits new point-values only if different from the previous point. Multiple fields can be monitored for change by this node. If no fields are given, the complete data-item is compared to the last one. If a reset_timeout is given, all previous values will be reset when no value is received within this amount of time. So that after the timeout the first data_item will be emitted. For value comparison erlang's strict equals (=:=) is used, so 1.0 is not equal to 1.","title":"The change_detect node"},{"location":"nodes/logic/change_detect.html#example","text":"%% detects any changes in data_items | change_detect() %% detect changes in one field, with timeout %% outputs at least the first data_item coming in after a 3 second timeout | change_detect( 'val' ) . reset_timeout ( 3s ) %% detect changes in two fields, with timeout | change_detect( 'data.State.Err' , 'data.State.Msg' ) . reset_timeout ( 60s ) % in-example json notation: % {\"data\": {\"x\": {\"temp\": 32.4564}, \"y\" : {\"temp\" : 31.15155}} } | change_detect( 'data.x.temp' , 'data.y.temp' )","title":"Example"},{"location":"nodes/logic/change_detect.html#parameters","text":"Parameter Description Default [node] fields( string_list ) List of fields to monitor optional reset_timeout( duration ) Previous values TTL 3h","title":"Parameters"},{"location":"nodes/logic/deadman.html","text":"The deadman node Emits a point, if there is no point coming in for the given amount of time. For output there are two options: If repeat_last param is set, the node will output the last message it saw incoming as the dead-message, if there is no last message yet, an empty message will be emitted With fields and field_values a list of values can be provided to be included in the output. If no fields (and field_values) parameter and is given, an empty datapoint will be emitted. The repeat_last parameter will always override the fields and field_values parameter The node will forward every message it gets by default, this can be changed by using the no_forward flag Example | deadman( 15s ) Parameters Parameter Description Default [node] timeout( duration ) timeout value for the node fields( string_list ) undefined field_values ( string_list ) undefined repeat_last( is_set) whether to output the last value seen false, not set no_forward( is_set) whether to output every message that comes in (pass through) false, not set","title":"Deadman"},{"location":"nodes/logic/deadman.html#the-deadman-node","text":"Emits a point, if there is no point coming in for the given amount of time. For output there are two options: If repeat_last param is set, the node will output the last message it saw incoming as the dead-message, if there is no last message yet, an empty message will be emitted With fields and field_values a list of values can be provided to be included in the output. If no fields (and field_values) parameter and is given, an empty datapoint will be emitted. The repeat_last parameter will always override the fields and field_values parameter The node will forward every message it gets by default, this can be changed by using the no_forward flag","title":"The deadman node"},{"location":"nodes/logic/deadman.html#example","text":"| deadman( 15s )","title":"Example"},{"location":"nodes/logic/deadman.html#parameters","text":"Parameter Description Default [node] timeout( duration ) timeout value for the node fields( string_list ) undefined field_values ( string_list ) undefined repeat_last( is_set) whether to output the last value seen false, not set no_forward( is_set) whether to output every message that comes in (pass through) false, not set","title":"Parameters"},{"location":"nodes/logic/eval.html","text":"The eval node Evaluate one or more lambda expressions. For an explanation of lambdas, see lambda . The list of lambda expressions given, will be evaluated top down. This means that a lambda can use the result of a previous expression. The resulting fields named with the as parameter will be added to the current data-point. Examples | eval( lambda : \"val\" * 2 , lambda : \"double\" / 2 ) %% 'double' is also used in the second expression above . as ( 'double' , 'val' ) This example demonstrates the 'serial' behaviour of the eval node. The second expression uses the field double , which the first expression just created. | eval( lambda : int ( str_concat ( string ( int ( \"val\" )), string ( int ( \"val\" )))) ) . as ( 'concat_string.int' ) The string value of the field 'val' is concatenated to itself, this is then casted to an int value and added to the current data-point as the field 'concat_string.int'. For more lambda examples see lambda Parameters Parameter Description Default [node] lambdas( lambda_list ) list of lambda expressions as( string_list ) list of output fieldnames (must have the same length as lambdas ) tags ( string_list ) list of output tagnames []","title":"Eval"},{"location":"nodes/logic/eval.html#the-eval-node","text":"Evaluate one or more lambda expressions. For an explanation of lambdas, see lambda . The list of lambda expressions given, will be evaluated top down. This means that a lambda can use the result of a previous expression. The resulting fields named with the as parameter will be added to the current data-point.","title":"The eval node"},{"location":"nodes/logic/eval.html#examples","text":"| eval( lambda : \"val\" * 2 , lambda : \"double\" / 2 ) %% 'double' is also used in the second expression above . as ( 'double' , 'val' ) This example demonstrates the 'serial' behaviour of the eval node. The second expression uses the field double , which the first expression just created. | eval( lambda : int ( str_concat ( string ( int ( \"val\" )), string ( int ( \"val\" )))) ) . as ( 'concat_string.int' ) The string value of the field 'val' is concatenated to itself, this is then casted to an int value and added to the current data-point as the field 'concat_string.int'. For more lambda examples see lambda","title":"Examples"},{"location":"nodes/logic/eval.html#parameters","text":"Parameter Description Default [node] lambdas( lambda_list ) list of lambda expressions as( string_list ) list of output fieldnames (must have the same length as lambdas ) tags ( string_list ) list of output tagnames []","title":"Parameters"},{"location":"nodes/logic/sample.html","text":"The sample node Samples the incoming points or batches. One point will be emitted every count or duration specified. When a duration is given, this node will emit the first data-item arriving after the timeout, then the timeout starts again. Example | sample( 5 ) Keep every 5th data_point or data_batch. | sample( 10s ) Keep the first point or batch after a 10 second interval. Parameters Parameter Description Default [node] rate ( integer duration ) sample rate","title":"Sample"},{"location":"nodes/logic/sample.html#the-sample-node","text":"Samples the incoming points or batches. One point will be emitted every count or duration specified. When a duration is given, this node will emit the first data-item arriving after the timeout, then the timeout starts again.","title":"The sample node"},{"location":"nodes/logic/sample.html#example","text":"| sample( 5 ) Keep every 5th data_point or data_batch. | sample( 10s ) Keep the first point or batch after a 10 second interval.","title":"Example"},{"location":"nodes/logic/sample.html#parameters","text":"Parameter Description Default [node] rate ( integer duration ) sample rate","title":"Parameters"},{"location":"nodes/logic/shift.html","text":"The shift node The shift node shifts points and batches in time. This is useful for comparing batches or points from different times. Example | shift( 5m ) Shift all data points 5 minutes forward in time. | shift( - 10s ) Shift all data points 10 seconds backwards in time. Parameters Parameter Description Default [node] offset ( duration ) time offset","title":"Shift"},{"location":"nodes/logic/shift.html#the-shift-node","text":"The shift node shifts points and batches in time. This is useful for comparing batches or points from different times.","title":"The shift node"},{"location":"nodes/logic/shift.html#example","text":"| shift( 5m ) Shift all data points 5 minutes forward in time. | shift( - 10s ) Shift all data points 10 seconds backwards in time.","title":"Example"},{"location":"nodes/logic/shift.html#parameters","text":"Parameter Description Default [node] offset ( duration ) time offset","title":"Parameters"},{"location":"nodes/logic/state_change.html","text":"The state_change node Computes the duration and count of points of a given state. The state is defined via a lambda expression. For each consecutive point for which the expression evaluates as true, state count and duration will be incremented. This node can be used to track state in data and produce new data based on the state-events. It can produce new data-points every time the state is entered and/or left . The enter data-point If the .enter() option is set, a new data-point will be emitted on state-enter. The new data-point will have a field, named with the .enter_as() option, set to 1 . This fieldname defaults to state_entered . The leave data-point If the .leave() option is set, a new data-point will be emitted on state-leave. Fields for this data-point: Name Description state_left stateflag, set to 1 state_start_ts timestamp at which the state has been entered state_end_ts timestamp at which the state-expression has been satisfied the last time state_duration duration of the state in milliseconds state_count number of points, the number of consecutive data-points for which the state-expression returned true When the lambda expression generates an error during evaluation, the current point is discarded and does not affect any calculations. Note that while state-count is 1, state-duration will be 0, if there is exactly 1 data-point within the state-window. Example %% the lambda defines our state | state_change ( lambda : \"val\" < 7 AND \"err\" != 0 ) %% the node will emit a data-point on state-leave only . leave () %% we keep these fields for the new state-leave data-point . leave_keep ( 'err' , 'err_code' ) Example output in json: { \"ts\" : 1232154654655 , \"err\" : 1 , \"err_code\" : 1492 , \"state_left\" : 1 , \"state_start_ts\" : 1232154644655 , \"state_end_ts\" : 1232154654655 , \"state_duration\" : 10000 , \"state_count\" : 22 } %% the lambda defines our state | state_change ( lambda : \"val\" > 2 OR \"err\" == 1 ) %% the node will emit a data-point on state-enter . enter () %% the node will emit a data-point on state-leave . leave () %% we keep these fields for the new state-enter data-point . enter_keep ( 'err' , 'err_code' ) %% we keep these fields for the new state-leave data-point . leave_keep ( 'err' , 'err_code' ) %% prefix all fields written by this node with 'my_' . prefix ( 'my_' ) Example output in json for the enter data-point: { \"ts\" : 1232154654655 , \"err\" : 1 , \"err_code\" : 1492 , \"my_state_entered\" : 1 } Parameters Parameter Description Default [node] lambda( lambda ) state lambda expression enter( is_set ) emit a datapoint on state-enter undefined leave( is_set ) emit a datapoint on state-leave undefined enter_as( string ) name for the \"enter\" field, it will be set to true 'state_entered' leave_as( string ) name for the \"leave\" field, it will be set to true 'state_left' enter_keep( string_list ) a list of fieldnames that should be kept for the enter data-point [] leave_keep( string_list ) a list of fieldnames that should be kept for the leave data-point [] prefix( string ) prefix fields added by this node with a string ( keep -fields stay untouched) '' (empty string) At least one of the enter | leave options must be given.","title":"State change"},{"location":"nodes/logic/state_change.html#the-state_change-node","text":"Computes the duration and count of points of a given state. The state is defined via a lambda expression. For each consecutive point for which the expression evaluates as true, state count and duration will be incremented. This node can be used to track state in data and produce new data based on the state-events. It can produce new data-points every time the state is entered and/or left .","title":"The state_change node"},{"location":"nodes/logic/state_change.html#the-enter-data-point","text":"If the .enter() option is set, a new data-point will be emitted on state-enter. The new data-point will have a field, named with the .enter_as() option, set to 1 . This fieldname defaults to state_entered .","title":"The enter data-point"},{"location":"nodes/logic/state_change.html#the-leave-data-point","text":"If the .leave() option is set, a new data-point will be emitted on state-leave. Fields for this data-point: Name Description state_left stateflag, set to 1 state_start_ts timestamp at which the state has been entered state_end_ts timestamp at which the state-expression has been satisfied the last time state_duration duration of the state in milliseconds state_count number of points, the number of consecutive data-points for which the state-expression returned true When the lambda expression generates an error during evaluation, the current point is discarded and does not affect any calculations. Note that while state-count is 1, state-duration will be 0, if there is exactly 1 data-point within the state-window.","title":"The leave data-point"},{"location":"nodes/logic/state_change.html#example","text":"%% the lambda defines our state | state_change ( lambda : \"val\" < 7 AND \"err\" != 0 ) %% the node will emit a data-point on state-leave only . leave () %% we keep these fields for the new state-leave data-point . leave_keep ( 'err' , 'err_code' ) Example output in json: { \"ts\" : 1232154654655 , \"err\" : 1 , \"err_code\" : 1492 , \"state_left\" : 1 , \"state_start_ts\" : 1232154644655 , \"state_end_ts\" : 1232154654655 , \"state_duration\" : 10000 , \"state_count\" : 22 } %% the lambda defines our state | state_change ( lambda : \"val\" > 2 OR \"err\" == 1 ) %% the node will emit a data-point on state-enter . enter () %% the node will emit a data-point on state-leave . leave () %% we keep these fields for the new state-enter data-point . enter_keep ( 'err' , 'err_code' ) %% we keep these fields for the new state-leave data-point . leave_keep ( 'err' , 'err_code' ) %% prefix all fields written by this node with 'my_' . prefix ( 'my_' ) Example output in json for the enter data-point: { \"ts\" : 1232154654655 , \"err\" : 1 , \"err_code\" : 1492 , \"my_state_entered\" : 1 }","title":"Example"},{"location":"nodes/logic/state_change.html#parameters","text":"Parameter Description Default [node] lambda( lambda ) state lambda expression enter( is_set ) emit a datapoint on state-enter undefined leave( is_set ) emit a datapoint on state-leave undefined enter_as( string ) name for the \"enter\" field, it will be set to true 'state_entered' leave_as( string ) name for the \"leave\" field, it will be set to true 'state_left' enter_keep( string_list ) a list of fieldnames that should be kept for the enter data-point [] leave_keep( string_list ) a list of fieldnames that should be kept for the leave data-point [] prefix( string ) prefix fields added by this node with a string ( keep -fields stay untouched) '' (empty string) At least one of the enter | leave options must be given.","title":"Parameters"},{"location":"nodes/logic/state_count.html","text":"The state_count node Computes the number of consecutive points in a given state. The state is defined via a lambda expression. For each consecutive point for which the expression evaluates as true, the state count will be incremented. When a point evaluates to false, the state count is reset. The state count will be added as an additional int field to each point. If the expression evaluates to false, the value will be -1. If the expression generates an error during evaluation, the point is discarded and does not affect the state count. Example | state_count( lambda : \"val\" < 7 ) . as ( 'val_below_7' ) Counts the number of consecutive points which have the value of the val field below 7 . Parameters Parameter Description Default [node] lambda( lambda ) state lambda expression as( string ) name for the added count field 'state_count'","title":"State count"},{"location":"nodes/logic/state_count.html#the-state_count-node","text":"Computes the number of consecutive points in a given state. The state is defined via a lambda expression. For each consecutive point for which the expression evaluates as true, the state count will be incremented. When a point evaluates to false, the state count is reset. The state count will be added as an additional int field to each point. If the expression evaluates to false, the value will be -1. If the expression generates an error during evaluation, the point is discarded and does not affect the state count.","title":"The state_count node"},{"location":"nodes/logic/state_count.html#example","text":"| state_count( lambda : \"val\" < 7 ) . as ( 'val_below_7' ) Counts the number of consecutive points which have the value of the val field below 7 .","title":"Example"},{"location":"nodes/logic/state_count.html#parameters","text":"Parameter Description Default [node] lambda( lambda ) state lambda expression as( string ) name for the added count field 'state_count'","title":"Parameters"},{"location":"nodes/logic/state_duration.html","text":"The state_duration node Computes the duration of a given state. The state is defined via a lambda expression. For each consecutive point for which the lambda expression evaluates as true, the state duration will be incremented by the duration between points. When a point evaluates as false, the state duration is reset. The state duration will be added as an additional field to each point and it's unit is milliseconds . If the expression evaluates to false, the value will be -1. When the lambda expression generates an error during evaluation, the point is discarded and does not affect the state duration.. Example | state_duration( lambda : \"val\" < 7 ) Parameters Parameter Description Default [node] lambda( lambda ) state lambda expression as( string ) name for the added duration field 'state_duration'","title":"State duration"},{"location":"nodes/logic/state_duration.html#the-state_duration-node","text":"Computes the duration of a given state. The state is defined via a lambda expression. For each consecutive point for which the lambda expression evaluates as true, the state duration will be incremented by the duration between points. When a point evaluates as false, the state duration is reset. The state duration will be added as an additional field to each point and it's unit is milliseconds . If the expression evaluates to false, the value will be -1. When the lambda expression generates an error during evaluation, the point is discarded and does not affect the state duration..","title":"The state_duration node"},{"location":"nodes/logic/state_duration.html#example","text":"| state_duration( lambda : \"val\" < 7 )","title":"Example"},{"location":"nodes/logic/state_duration.html#parameters","text":"Parameter Description Default [node] lambda( lambda ) state lambda expression as( string ) name for the added duration field 'state_duration'","title":"Parameters"},{"location":"nodes/logic/state_sequence.html","text":"The state_sequence node This node takes a list of lambda expressions representing different states. It will emit values only after each state has evaluated as true in the given order and, for each step in the sequence within the corresponding timeout. A transition timeout must be defined for every state transition with the within parameter. If a timeout occurs at any point the sequence will be reset and started from the first expression again. Note that the sequence timeouts start after the first data_point has satisfied the first lambda expression. Therefore, if 3 lambda states are given, only 2 durations for the within parameter can be defined. With the strict parameter the sequence of states must be met exactly without any intermediary data_points coming in, that do not satisfy the current state expression. Normally this would not reset the sequence of evaluation, in this mode, it will. On a successful evaluation of the whole sequence, the node will simply output the last value, that completed the sequence. The state_sequence node can be used with one or many input nodes. Example in1 | state_sequence( in2 , in3 ) %% can use any number of nodes . states ( lambda : \"data.topic\" == 'in1' , %% state 1 lambda : \"data.topic\" == 'in2' , %% state 2 lambda : \"data.topic\" == 'in3' %% state 3 ) . within ( 25s , %% transition-time from state 1 to state 2 20s %% transition-time from state 2 to state 3 ) Parameters Parameter Description Default [node] nodes_in( node_list ) a list of node(chains) optional states ( lambda_list ) the states within( duration_list ) one timeout for every state-transition strict( is_set ) whether the state sequence must be transition exactly false (not set)","title":"State sequence"},{"location":"nodes/logic/state_sequence.html#the-state_sequence-node","text":"This node takes a list of lambda expressions representing different states. It will emit values only after each state has evaluated as true in the given order and, for each step in the sequence within the corresponding timeout. A transition timeout must be defined for every state transition with the within parameter. If a timeout occurs at any point the sequence will be reset and started from the first expression again. Note that the sequence timeouts start after the first data_point has satisfied the first lambda expression. Therefore, if 3 lambda states are given, only 2 durations for the within parameter can be defined. With the strict parameter the sequence of states must be met exactly without any intermediary data_points coming in, that do not satisfy the current state expression. Normally this would not reset the sequence of evaluation, in this mode, it will. On a successful evaluation of the whole sequence, the node will simply output the last value, that completed the sequence. The state_sequence node can be used with one or many input nodes.","title":"The state_sequence node"},{"location":"nodes/logic/state_sequence.html#example","text":"in1 | state_sequence( in2 , in3 ) %% can use any number of nodes . states ( lambda : \"data.topic\" == 'in1' , %% state 1 lambda : \"data.topic\" == 'in2' , %% state 2 lambda : \"data.topic\" == 'in3' %% state 3 ) . within ( 25s , %% transition-time from state 1 to state 2 20s %% transition-time from state 2 to state 3 )","title":"Example"},{"location":"nodes/logic/state_sequence.html#parameters","text":"Parameter Description Default [node] nodes_in( node_list ) a list of node(chains) optional states ( lambda_list ) the states within( duration_list ) one timeout for every state-transition strict( is_set ) whether the state sequence must be transition exactly false (not set)","title":"Parameters"},{"location":"nodes/logic/time_diff.html","text":"The time_diff node The time_diff node adds a field to the current data-item containing the difference between the timestamps of the consecutive items. Note that the difference in time will be calculated from the data-points timestamp fields and does not reflect the difference in time points coming into the node. For the other behaviour see time_elapsed . The unit for output values is milliseconds. Example | time_diff() . as ( 'time_diff' ) Parameters Parameter Description Default as( string ) name of the field for parsed data 'timediff'","title":"Time diff"},{"location":"nodes/logic/time_diff.html#the-time_diff-node","text":"The time_diff node adds a field to the current data-item containing the difference between the timestamps of the consecutive items. Note that the difference in time will be calculated from the data-points timestamp fields and does not reflect the difference in time points coming into the node. For the other behaviour see time_elapsed . The unit for output values is milliseconds.","title":"The time_diff node"},{"location":"nodes/logic/time_diff.html#example","text":"| time_diff() . as ( 'time_diff' )","title":"Example"},{"location":"nodes/logic/time_diff.html#parameters","text":"Parameter Description Default as( string ) name of the field for parsed data 'timediff'","title":"Parameters"},{"location":"nodes/logic/time_elapsed.html","text":"The time_elapsed node The time_elapsed node adds a field to the current data-item containing the difference in arrival time of consecutive items. See the time_diff node . The unit for output values is milliseconds. Example | time_elapsed() . as ( 'time_dur' ) Parameters Parameter Description Default as( string ) name of the field for parsed data 'elapsed'","title":"Time elapsed"},{"location":"nodes/logic/time_elapsed.html#the-time_elapsed-node","text":"The time_elapsed node adds a field to the current data-item containing the difference in arrival time of consecutive items. See the time_diff node . The unit for output values is milliseconds.","title":"The time_elapsed node"},{"location":"nodes/logic/time_elapsed.html#example","text":"| time_elapsed() . as ( 'time_dur' )","title":"Example"},{"location":"nodes/logic/time_elapsed.html#parameters","text":"Parameter Description Default as( string ) name of the field for parsed data 'elapsed'","title":"Parameters"},{"location":"nodes/logic/triggered_timeout.html","text":"The triggered_timeout node Emits a point, if there is no message coming in for the given amount of time. A timeout will be started on an explicit trigger: * When a lambda expression is given for parameter timeout_trigger , this expression must evaluate as true to start (and after a timeout has occurred to restart) a timeout. If no lambda expression is given for the timeout_trigger , the trigger is any data_point coming in on port 1, the so called trigger_port . A new trigger does not restart a running timeout. After a timeout occurred, the node waits for a new trigger to come in before it starts a new timeout. After a timeout is started the node waits for data coming in, that either does not satisfy the trigger expression(when a lambda expression is given for the timeout_trigger parameter) or is coming in on any port except the trigger_port (port 1). Data for the outgoing data-point can be defined with the fields and field_values parameters. This node can have any number of input-nodes. Example def timeout = 30s % ... in1 | triggered_timeout( in2 ) . timeout ( timeout ) . timeout_trigger ( lambda : \"data.topic\" == 'in1' ) def condition_reason = 'oh no !!' robot_state | triggered_timeout( or derlog ) . timeout ( timeout ) . fields ( 'combined.condition.name' , 'combined.condition_reason' , 'combined.condition.id' ) . field_values ( 'ERROR' , condition_reason , 2 ) %.cancel_fields('combined.condition.name', 'combined.condition_reason', 'combined.condition.id') %.cancel_field_values('OK', '', 0) Parameters Parameter Description Default timeout( duration ) timeout_trigger( lambda ) lambda expression which triggers the timeout optional fields ( string_list ) paths for the output fields optional field_values( list ) values for the output fields optional","title":"Triggered timeout"},{"location":"nodes/logic/triggered_timeout.html#the-triggered_timeout-node","text":"Emits a point, if there is no message coming in for the given amount of time. A timeout will be started on an explicit trigger: * When a lambda expression is given for parameter timeout_trigger , this expression must evaluate as true to start (and after a timeout has occurred to restart) a timeout. If no lambda expression is given for the timeout_trigger , the trigger is any data_point coming in on port 1, the so called trigger_port . A new trigger does not restart a running timeout. After a timeout occurred, the node waits for a new trigger to come in before it starts a new timeout. After a timeout is started the node waits for data coming in, that either does not satisfy the trigger expression(when a lambda expression is given for the timeout_trigger parameter) or is coming in on any port except the trigger_port (port 1). Data for the outgoing data-point can be defined with the fields and field_values parameters. This node can have any number of input-nodes.","title":"The triggered_timeout node"},{"location":"nodes/logic/triggered_timeout.html#example","text":"def timeout = 30s % ... in1 | triggered_timeout( in2 ) . timeout ( timeout ) . timeout_trigger ( lambda : \"data.topic\" == 'in1' ) def condition_reason = 'oh no !!' robot_state | triggered_timeout( or derlog ) . timeout ( timeout ) . fields ( 'combined.condition.name' , 'combined.condition_reason' , 'combined.condition.id' ) . field_values ( 'ERROR' , condition_reason , 2 ) %.cancel_fields('combined.condition.name', 'combined.condition_reason', 'combined.condition.id') %.cancel_field_values('OK', '', 0)","title":"Example"},{"location":"nodes/logic/triggered_timeout.html#parameters","text":"Parameter Description Default timeout( duration ) timeout_trigger( lambda ) lambda expression which triggers the timeout optional fields ( string_list ) paths for the output fields optional field_values( list ) values for the output fields optional","title":"Parameters"},{"location":"nodes/logic/where.html","text":"The where node Filter points and batches with a lambda expression, which returns a boolean value. Data-items for which the lambda expression evaluates as false will be discarded. Example | where( lambda : hour ( \"ts\" ) < 18 AND hour ( \"ts\" ) > 8 ) Filters points who's timestamp is not between 09:00 and 18:00. Parameters Parameter Description Default [node] lambda( lambda ) The lambda filter expression","title":"Where"},{"location":"nodes/logic/where.html#the-where-node","text":"Filter points and batches with a lambda expression, which returns a boolean value. Data-items for which the lambda expression evaluates as false will be discarded.","title":"The where node"},{"location":"nodes/logic/where.html#example","text":"| where( lambda : hour ( \"ts\" ) < 18 AND hour ( \"ts\" ) > 8 ) Filters points who's timestamp is not between 09:00 and 18:00.","title":"Example"},{"location":"nodes/logic/where.html#parameters","text":"Parameter Description Default [node] lambda( lambda ) The lambda filter expression","title":"Parameters"},{"location":"nodes/messaging/amqp_consume.html","text":"The amqp_consume node Consume data from an amqp-broker like rabbitmq. When prefetch is given and is > 1, then this node will emit a data_batch instead of a data_point. Example | amqp_consume() . host ( 'deves-amqp-cluster1.internal' ) . routing_key ( 'my.routing.key' ) . exchange ( 'x_xchange' ) . queue ( 'faxe_test' ) . dt_field ( 'UTC-Time' ) . dt_format ( 'float_micro' ) Parameters Parameter Description Default host( string ) Ip address or hostname of the broker port( integer ) The broker's port 5672 / from config file user( string ) AMQP user from config file pass( string ) AMQP password from config file vhost( string ) vhost to connect to on the broker '/' routing_key( string ) routing key to use for queue binding queue( string ) name of the queue to bind to the exchange exchange( string ) name of the exchange to bind to the source exchange prefetch( integer ) prefetch count to use 1 dt_field( string ) name of the timestamp field that is expected 'ts' dt_format( string ) timestamp or datetime format that is expected (see table below) 'millisecond' ssl( is_set ) whether to use ssl false (not set) Available datetime formats dt_format description example 'millisecond' timestamp UTC in milliseconds 1565343079000 'second' timestamp UTC in seconds 1565343079 'float_micro' timestamp UTC float with microsecond precision 1565343079.173588 'float_millisecond' timestamp UTC float with millisecond precision 1565343079.173 'ISO8601' ISO8601 Datetime format string '2011-10-05T14:48:00.000Z' 'RFC3339' RFC3339 Datetime format string '2018-02-01 15:18:02.088Z' 'convtrack_datetime' special datetime format used in the conveyor tracking data stream '19.08.01 17:33:44,867 '","title":"Amqp consume"},{"location":"nodes/messaging/amqp_consume.html#the-amqp_consume-node","text":"Consume data from an amqp-broker like rabbitmq. When prefetch is given and is > 1, then this node will emit a data_batch instead of a data_point.","title":"The amqp_consume node"},{"location":"nodes/messaging/amqp_consume.html#example","text":"| amqp_consume() . host ( 'deves-amqp-cluster1.internal' ) . routing_key ( 'my.routing.key' ) . exchange ( 'x_xchange' ) . queue ( 'faxe_test' ) . dt_field ( 'UTC-Time' ) . dt_format ( 'float_micro' )","title":"Example"},{"location":"nodes/messaging/amqp_consume.html#parameters","text":"Parameter Description Default host( string ) Ip address or hostname of the broker port( integer ) The broker's port 5672 / from config file user( string ) AMQP user from config file pass( string ) AMQP password from config file vhost( string ) vhost to connect to on the broker '/' routing_key( string ) routing key to use for queue binding queue( string ) name of the queue to bind to the exchange exchange( string ) name of the exchange to bind to the source exchange prefetch( integer ) prefetch count to use 1 dt_field( string ) name of the timestamp field that is expected 'ts' dt_format( string ) timestamp or datetime format that is expected (see table below) 'millisecond' ssl( is_set ) whether to use ssl false (not set)","title":"Parameters"},{"location":"nodes/messaging/amqp_consume.html#available-datetime-formats","text":"dt_format description example 'millisecond' timestamp UTC in milliseconds 1565343079000 'second' timestamp UTC in seconds 1565343079 'float_micro' timestamp UTC float with microsecond precision 1565343079.173588 'float_millisecond' timestamp UTC float with millisecond precision 1565343079.173 'ISO8601' ISO8601 Datetime format string '2011-10-05T14:48:00.000Z' 'RFC3339' RFC3339 Datetime format string '2018-02-01 15:18:02.088Z' 'convtrack_datetime' special datetime format used in the conveyor tracking data stream '19.08.01 17:33:44,867 '","title":"Available datetime formats"},{"location":"nodes/messaging/amqp_publish.html","text":"The amqp_publish node Publish data to an amqp-broker exchange like rabbitmq. Incoming data is converted to JSON before sending. Example | amqp_publish() . host ( '127.0.0.1' ) . routing_key ( 'my.routing.key' ) . exchange ( 'x_xchange' ) Parameters Parameter Description Default host( string ) Ip address or hostname of the broker port( integer ) The broker's port 5672 / from config file user( string ) AMQP user from config file pass( string ) AMQP password from config file vhost( string ) vhost to connect to on the broker '/' routing_key( string ) routing key for the published messages exchange( string ) name of the exchange to publish to ssl( is_set ) whether to use ssl false (not set)","title":"Amqp publish"},{"location":"nodes/messaging/amqp_publish.html#the-amqp_publish-node","text":"Publish data to an amqp-broker exchange like rabbitmq. Incoming data is converted to JSON before sending.","title":"The amqp_publish node"},{"location":"nodes/messaging/amqp_publish.html#example","text":"| amqp_publish() . host ( '127.0.0.1' ) . routing_key ( 'my.routing.key' ) . exchange ( 'x_xchange' )","title":"Example"},{"location":"nodes/messaging/amqp_publish.html#parameters","text":"Parameter Description Default host( string ) Ip address or hostname of the broker port( integer ) The broker's port 5672 / from config file user( string ) AMQP user from config file pass( string ) AMQP password from config file vhost( string ) vhost to connect to on the broker '/' routing_key( string ) routing key for the published messages exchange( string ) name of the exchange to publish to ssl( is_set ) whether to use ssl false (not set)","title":"Parameters"},{"location":"nodes/messaging/mqtt_publish.html","text":"The mqtt_publish node Publish data to an mqtt-broker. Incoming data is converted to JSON before sending. If the save() parameter is given, every message first gets stored to an on-disk queue before sending, this way we can make sure no message gets lost when disconnected from the broker. Example def topic = 'top/track/pressure' | mqtt_publish() . topic ( topic ) . retained () Using a lambda expression for the topic: def topic_base = 'top/' | mqtt_publish() . topic_lambda ( lambda : str_concat ([ topic_base , \"type\" , '/' , \"measurement\" ]) Here the topic string is built with a lambda expression using the topic_base declaration, the string '/' and two fields from the current data_point. The topic string may be a different one with every data_point that gets published. Parameters Parameter Description Default host( string ) Ip address or hostname of the broker from config file port( integer ) The broker's port 1883 from config file user( string ) username from config file pass( string ) password from config file topic( string ) mqtt topic to use topic_lambda( lambda ) mqtt topic to use evaluated via a lambda expression qos( integer ) Quality of service, one of 0, 1 or 2 1 retained( is_set ) whether the message should be retained on the broker false (not set) save( is_set ) send save (on-disk queuing) false (not set) ssl( is_set ) whether to use ssl false (not set) topic or topic_lambda must be provided.","title":"Mqtt publish"},{"location":"nodes/messaging/mqtt_publish.html#the-mqtt_publish-node","text":"Publish data to an mqtt-broker. Incoming data is converted to JSON before sending. If the save() parameter is given, every message first gets stored to an on-disk queue before sending, this way we can make sure no message gets lost when disconnected from the broker.","title":"The mqtt_publish node"},{"location":"nodes/messaging/mqtt_publish.html#example","text":"def topic = 'top/track/pressure' | mqtt_publish() . topic ( topic ) . retained () Using a lambda expression for the topic: def topic_base = 'top/' | mqtt_publish() . topic_lambda ( lambda : str_concat ([ topic_base , \"type\" , '/' , \"measurement\" ]) Here the topic string is built with a lambda expression using the topic_base declaration, the string '/' and two fields from the current data_point. The topic string may be a different one with every data_point that gets published.","title":"Example"},{"location":"nodes/messaging/mqtt_publish.html#parameters","text":"Parameter Description Default host( string ) Ip address or hostname of the broker from config file port( integer ) The broker's port 1883 from config file user( string ) username from config file pass( string ) password from config file topic( string ) mqtt topic to use topic_lambda( lambda ) mqtt topic to use evaluated via a lambda expression qos( integer ) Quality of service, one of 0, 1 or 2 1 retained( is_set ) whether the message should be retained on the broker false (not set) save( is_set ) send save (on-disk queuing) false (not set) ssl( is_set ) whether to use ssl false (not set) topic or topic_lambda must be provided.","title":"Parameters"},{"location":"nodes/messaging/mqtt_subscribe.html","text":"The mqtt_subscribe node Subscribe to an mqtt-broker and get data from one or more topics. Example | mqtt_subscribe() . topics ( 'top/grips/#' ) . dt_field ( 'UTC-Stamp' ) . dt_format ( 'float_micro' ) Parameters Parameter Description Default host( string ) Ip address or hostname of the broker from config file port( integer ) The broker's port 1883 from config file user( string ) username from config file pass( string ) password from config file topics( string_list ) mqtt topic to use qos( integer ) Quality of service, one of 0, 1 or 2 1 retained( is_set ) whether the message should be retained on the broker false (not set) dt_field( string ) name of the timestamp field that is expected 'ts' dt_format( string ) timestamp or datetime format that is expected (see table below) 'millisecond' ssl( is_set ) whether to use ssl false (not set) Available datetime formats dt_format description example 'millisecond' timestamp UTC in milliseconds 1565343079000 'second' timestamp UTC in seconds 1565343079 'float_micro' timestamp UTC float with microsecond precision 1565343079.173588 'float_millisecond' timestamp UTC float with millisecond precision 1565343079.173 'ISO8601' ISO8601 Datetime format string '2011-10-05T14:48:00.000Z' 'RFC3339' RFC3339 Datetime format string '2018-02-01 15:18:02.088Z' 'convtrack_datetime' special datetime format used in the conveyor tracking data stream '19.08.01 17:33:44,867 '","title":"Mqtt subscribe"},{"location":"nodes/messaging/mqtt_subscribe.html#the-mqtt_subscribe-node","text":"Subscribe to an mqtt-broker and get data from one or more topics.","title":"The mqtt_subscribe node"},{"location":"nodes/messaging/mqtt_subscribe.html#example","text":"| mqtt_subscribe() . topics ( 'top/grips/#' ) . dt_field ( 'UTC-Stamp' ) . dt_format ( 'float_micro' )","title":"Example"},{"location":"nodes/messaging/mqtt_subscribe.html#parameters","text":"Parameter Description Default host( string ) Ip address or hostname of the broker from config file port( integer ) The broker's port 1883 from config file user( string ) username from config file pass( string ) password from config file topics( string_list ) mqtt topic to use qos( integer ) Quality of service, one of 0, 1 or 2 1 retained( is_set ) whether the message should be retained on the broker false (not set) dt_field( string ) name of the timestamp field that is expected 'ts' dt_format( string ) timestamp or datetime format that is expected (see table below) 'millisecond' ssl( is_set ) whether to use ssl false (not set)","title":"Parameters"},{"location":"nodes/messaging/mqtt_subscribe.html#available-datetime-formats","text":"dt_format description example 'millisecond' timestamp UTC in milliseconds 1565343079000 'second' timestamp UTC in seconds 1565343079 'float_micro' timestamp UTC float with microsecond precision 1565343079.173588 'float_millisecond' timestamp UTC float with millisecond precision 1565343079.173 'ISO8601' ISO8601 Datetime format string '2011-10-05T14:48:00.000Z' 'RFC3339' RFC3339 Datetime format string '2018-02-01 15:18:02.088Z' 'convtrack_datetime' special datetime format used in the conveyor tracking data stream '19.08.01 17:33:44,867 '","title":"Available datetime formats"},{"location":"nodes/statistics/avg.html","text":"The avg node Compute the average. See the stats node Example | avg () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Avg"},{"location":"nodes/statistics/avg.html#the-avg-node","text":"Compute the average. See the stats node","title":"The avg node"},{"location":"nodes/statistics/avg.html#example","text":"| avg () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/avg.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/bottom.html","text":"The sum node Select the bottom num points for field. See the stats node Example | bottom () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node num( integer ) number of points to select 1","title":"Bottom"},{"location":"nodes/statistics/bottom.html#the-sum-node","text":"Select the bottom num points for field. See the stats node","title":"The sum node"},{"location":"nodes/statistics/bottom.html#example","text":"| bottom () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/bottom.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node num( integer ) number of points to select 1","title":"Parameters"},{"location":"nodes/statistics/count.html","text":"The count node Count the number of points. See the stats node Example | count () . field ( 'over_ts' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Count"},{"location":"nodes/statistics/count.html#the-count-node","text":"Count the number of points. See the stats node","title":"The count node"},{"location":"nodes/statistics/count.html#example","text":"| count () . field ( 'over_ts' )","title":"Example"},{"location":"nodes/statistics/count.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/distinct.html","text":"The distinct node Select unique values. See the stats node Example | distinct () . field ( 'status' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Distinct"},{"location":"nodes/statistics/distinct.html#the-distinct-node","text":"Select unique values. See the stats node","title":"The distinct node"},{"location":"nodes/statistics/distinct.html#example","text":"| distinct () . field ( 'status' )","title":"Example"},{"location":"nodes/statistics/distinct.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/elapsed.html","text":"The elapsed node Compute the elapsed time between points. See the stats node Example | elapsed () . field ( 'trigger' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Elapsed"},{"location":"nodes/statistics/elapsed.html#the-elapsed-node","text":"Compute the elapsed time between points. See the stats node","title":"The elapsed node"},{"location":"nodes/statistics/elapsed.html#example","text":"| elapsed () . field ( 'trigger' )","title":"Example"},{"location":"nodes/statistics/elapsed.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/first.html","text":"The first node Select the first that means the oldest point. See the stats node Example | first () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"First"},{"location":"nodes/statistics/first.html#the-first-node","text":"Select the first that means the oldest point. See the stats node","title":"The first node"},{"location":"nodes/statistics/first.html#example","text":"| first () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/first.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/geometric_mean.html","text":"The geometric_mean node Compute the geometric_mean. See the stats node Example | geometric_mean () . field ( 'pressure' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Geometric mean"},{"location":"nodes/statistics/geometric_mean.html#the-geometric_mean-node","text":"Compute the geometric_mean. See the stats node","title":"The geometric_mean node"},{"location":"nodes/statistics/geometric_mean.html#example","text":"| geometric_mean () . field ( 'pressure' )","title":"Example"},{"location":"nodes/statistics/geometric_mean.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/kurtosis.html","text":"The kurtosis node Compute the kurtosis of data. See the stats node Example | kurtosis () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Kurtosis"},{"location":"nodes/statistics/kurtosis.html#the-kurtosis-node","text":"Compute the kurtosis of data. See the stats node","title":"The kurtosis node"},{"location":"nodes/statistics/kurtosis.html#example","text":"| kurtosis () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/kurtosis.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/last.html","text":"The last node Select the last, that means the newest point. See the stats node Example | last () . field ( 'chair' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Last"},{"location":"nodes/statistics/last.html#the-last-node","text":"Select the last, that means the newest point. See the stats node","title":"The last node"},{"location":"nodes/statistics/last.html#example","text":"| last () . field ( 'chair' )","title":"Example"},{"location":"nodes/statistics/last.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/max.html","text":"The max node Compute the maximum value. See the stats node Example | max () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Max"},{"location":"nodes/statistics/max.html#the-max-node","text":"Compute the maximum value. See the stats node","title":"The max node"},{"location":"nodes/statistics/max.html#example","text":"| max () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/max.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/mean.html","text":"The mean node Compute the mean of data. See the stats node Example | mean () . field ( 'current' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Mean"},{"location":"nodes/statistics/mean.html#the-mean-node","text":"Compute the mean of data. See the stats node","title":"The mean node"},{"location":"nodes/statistics/mean.html#example","text":"| mean () . field ( 'current' )","title":"Example"},{"location":"nodes/statistics/mean.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/median.html","text":"The median node Compute the median of data. See the stats node Example | median () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Median"},{"location":"nodes/statistics/median.html#the-median-node","text":"Compute the median of data. See the stats node","title":"The median node"},{"location":"nodes/statistics/median.html#example","text":"| median () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/median.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/min.html","text":"The min node Compute the minimum of data. See the stats node Example | min () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Min"},{"location":"nodes/statistics/min.html#the-min-node","text":"Compute the minimum of data. See the stats node","title":"The min node"},{"location":"nodes/statistics/min.html#example","text":"| min () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/min.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/percentile.html","text":"The percentile node Select a point at the given percentile. This is a selector function, no interpolation between points is performed. See the stats node Example | percentile () . perc ( 95 ) . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node perc ( integer ) select percentile 95","title":"Percentile"},{"location":"nodes/statistics/percentile.html#the-percentile-node","text":"Select a point at the given percentile. This is a selector function, no interpolation between points is performed. See the stats node","title":"The percentile node"},{"location":"nodes/statistics/percentile.html#example","text":"| percentile () . perc ( 95 ) . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/percentile.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node perc ( integer ) select percentile 95","title":"Parameters"},{"location":"nodes/statistics/stddev.html","text":"The stddev node Compute the standard deviation of the data. See the stats node Example | stddev () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Stddev"},{"location":"nodes/statistics/stddev.html#the-stddev-node","text":"Compute the standard deviation of the data. See the stats node","title":"The stddev node"},{"location":"nodes/statistics/stddev.html#example","text":"| stddev () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/stddev.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/sum.html","text":"The sum node Compute the sum of data. See the stats node Example | sum () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Sum"},{"location":"nodes/statistics/sum.html#the-sum-node","text":"Compute the sum of data. See the stats node","title":"The sum node"},{"location":"nodes/statistics/sum.html#example","text":"| sum () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/sum.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/top.html","text":"The top node Select the top num points. See the stats node Example | top () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node num( integer ) number of points to select 1","title":"Top"},{"location":"nodes/statistics/top.html#the-top-node","text":"Select the top num points. See the stats node","title":"The top node"},{"location":"nodes/statistics/top.html#example","text":"| top () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/top.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node num( integer ) number of points to select 1","title":"Parameters"},{"location":"nodes/statistics/variance.html","text":"The variance node Compute the data's variance. See the stats node Example | variance () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Variance"},{"location":"nodes/statistics/variance.html#the-variance-node","text":"Compute the data's variance. See the stats node","title":"The variance node"},{"location":"nodes/statistics/variance.html#example","text":"| variance () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/variance.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/windows/win_clock.html","text":"The win_clock node A window node is for batching data_points. This window-type has wall-clock timing, timestamps contained in incoming events are not relevant here. When the align option is true, window boundaries are aligned according to the every option, this means when every is 5s and an event comes into the window at time 15:03:27, this event will be member of the window that starts at 15:03:25, otherwise the window would start at 15:03:27. By default, the boundries are defined relative to the first data point the window node receives. With fill_period given, the window will not emit before \"period\" time has elapsed (for the first time). This only applies if the period is greater than the every value. Example | win_clock() . every ( 5s ) . period ( 15s ) . fill_period () . align () The window will emit every 5 seconds, but only after initially 15 seconds have passed (due to fill_period ), it has its boundaries aligned to 5 second intervals. Parameters Parameter Description Default period( duration ) Window length defaults to every every( duration ) Output window contents every align( is_set ) Align the window boundaries false (not set) fill_period( is_set ) Window output only when period time has elapsed false (not set)","title":"Win clock"},{"location":"nodes/windows/win_clock.html#the-win_clock-node","text":"A window node is for batching data_points. This window-type has wall-clock timing, timestamps contained in incoming events are not relevant here. When the align option is true, window boundaries are aligned according to the every option, this means when every is 5s and an event comes into the window at time 15:03:27, this event will be member of the window that starts at 15:03:25, otherwise the window would start at 15:03:27. By default, the boundries are defined relative to the first data point the window node receives. With fill_period given, the window will not emit before \"period\" time has elapsed (for the first time). This only applies if the period is greater than the every value.","title":"The win_clock node"},{"location":"nodes/windows/win_clock.html#example","text":"| win_clock() . every ( 5s ) . period ( 15s ) . fill_period () . align () The window will emit every 5 seconds, but only after initially 15 seconds have passed (due to fill_period ), it has its boundaries aligned to 5 second intervals.","title":"Example"},{"location":"nodes/windows/win_clock.html#parameters","text":"Parameter Description Default period( duration ) Window length defaults to every every( duration ) Output window contents every align( is_set ) Align the window boundaries false (not set) fill_period( is_set ) Window output only when period time has elapsed false (not set)","title":"Parameters"},{"location":"nodes/windows/win_event.html","text":"The win_event node A window node is for batching data_points. This window holds period number of data_points and emits every every incoming point. With fill_period given, the window will only emit when it is filled with period points. This only applies if the period is greater than the every value. Examples | win_event() . every ( 5 ) . period ( 15 ) . fill_period () The window will emit it's contents every 5 incoming points, but only after the window is filled with 15 points. | win_event() . every ( 5 ) . period ( 15 ) The window will emit it's contents every 5 incoming points. On first emit 5 points will be outputted, on the second emit 10 points will be emitted. From the third emit onwards, the window will output 15 points. Starting with the 4th emit, the window will output 15 data_points - with 10 old and 5 new points (Tumbling window). Parameters Parameter Description Default period( integer ) Window length, number of points defaults to every every( integer ) Output window contents every n incoming points fill_period( is_set ) Output only when window is filled false (not set)","title":"Win event"},{"location":"nodes/windows/win_event.html#the-win_event-node","text":"A window node is for batching data_points. This window holds period number of data_points and emits every every incoming point. With fill_period given, the window will only emit when it is filled with period points. This only applies if the period is greater than the every value.","title":"The win_event node"},{"location":"nodes/windows/win_event.html#examples","text":"| win_event() . every ( 5 ) . period ( 15 ) . fill_period () The window will emit it's contents every 5 incoming points, but only after the window is filled with 15 points. | win_event() . every ( 5 ) . period ( 15 ) The window will emit it's contents every 5 incoming points. On first emit 5 points will be outputted, on the second emit 10 points will be emitted. From the third emit onwards, the window will output 15 points. Starting with the 4th emit, the window will output 15 data_points - with 10 old and 5 new points (Tumbling window).","title":"Examples"},{"location":"nodes/windows/win_event.html#parameters","text":"Parameter Description Default period( integer ) Window length, number of points defaults to every every( integer ) Output window contents every n incoming points fill_period( is_set ) Output only when window is filled false (not set)","title":"Parameters"},{"location":"nodes/windows/win_time.html","text":"The win_time node A window node is for batching data_points. This window refers it's timing to the timestamp contained in the incoming data-items. With fill_period given, the window will not emit before \"period\" time has elapsed (for the first time). Note that, since this window type does not rely on wall clock, but on the points timestamps, it is possible that no data is emitted, if there are no new points coming in. Example | win_time() . every ( 5s ) . period ( 15s ) The window will emit it's contents every 5 seconds. | win_time() . every ( 1m ) Period is 1 minute here (period defaults to every) Parameters Parameter Description Default period( duration ) Window length defaults to every every( duration ) Output window contents every fill_period( is_set ) Window output only when period time has accumulated false (not set)","title":"Win time"},{"location":"nodes/windows/win_time.html#the-win_time-node","text":"A window node is for batching data_points. This window refers it's timing to the timestamp contained in the incoming data-items. With fill_period given, the window will not emit before \"period\" time has elapsed (for the first time). Note that, since this window type does not rely on wall clock, but on the points timestamps, it is possible that no data is emitted, if there are no new points coming in.","title":"The win_time node"},{"location":"nodes/windows/win_time.html#example","text":"| win_time() . every ( 5s ) . period ( 15s ) The window will emit it's contents every 5 seconds. | win_time() . every ( 1m ) Period is 1 minute here (period defaults to every)","title":"Example"},{"location":"nodes/windows/win_time.html#parameters","text":"Parameter Description Default period( duration ) Window length defaults to every every( duration ) Output window contents every fill_period( is_set ) Window output only when period time has accumulated false (not set)","title":"Parameters"}]}